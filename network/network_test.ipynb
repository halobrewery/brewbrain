{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from recipedataset import RecipeDataset, RECIPE_DATASET_FILENAME, NUM_GRAIN_SLOTS, NUM_ADJUNCT_SLOTS, NUM_HOP_SLOTS, NUM_MISC_SLOTS, NUM_MICROORGANISM_SLOTS, NUM_FERMENT_STAGE_SLOTS, NUM_MASH_STEPS\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def layer_init_ortho(layer, std=np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, std)\n",
    "  nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def layer_init_xavier(layer, gain):\n",
    "  nn.init.xavier_normal_(layer.weight, gain)\n",
    "  nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "  std = torch.exp(0.5 * logvar)\n",
    "  eps = torch.randn_like(std)\n",
    "  return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Load the dataset and create a dataloader for it\n",
    "with open(\"../\" + RECIPE_DATASET_FILENAME, 'rb') as f:\n",
    "  dataset = pickle.load(f)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAIN_TYPE_EMBED_SIZE         = 32\n",
    "ADJUNCT_TYPE_EMBED_SIZE       = 32\n",
    "HOP_TYPE_EMBED_SIZE           = 96\n",
    "MISC_TYPE_EMBED_SIZE          = 64\n",
    "MICROORGANISM_TYPE_EMBED_SIZE = 64\n",
    "\n",
    "class RecipeNetArgs:\n",
    "  def __init__(self, dataset) -> None:\n",
    "    # Recipe-specific constraints ***\n",
    "    self.num_mash_steps          = NUM_MASH_STEPS\n",
    "    self.num_grain_slots         = NUM_GRAIN_SLOTS\n",
    "    self.num_adjunct_slots       = NUM_ADJUNCT_SLOTS\n",
    "    self.num_hop_slots           = NUM_HOP_SLOTS\n",
    "    self.num_misc_slots          = NUM_MISC_SLOTS\n",
    "    self.num_microorganism_slots = NUM_MICROORGANISM_SLOTS\n",
    "    self.num_ferment_stage_slots = NUM_FERMENT_STAGE_SLOTS\n",
    "    \n",
    "    # NOTE: All types include a \"None\" (i.e., empty) category\n",
    "    self.num_grain_types         = len(dataset.core_grains_idx_to_dbid) # Number of (core) grain types (rows in the DB)\n",
    "    self.num_adjunct_types       = len(dataset.core_adjs_idx_to_dbid)   # Number of (core) adjunct types (rows in the DB)\n",
    "    self.num_hop_types           = len(dataset.hops_idx_to_dbid)        # Number of hop types (rows in the DB)\n",
    "    self.num_misc_types          = len(dataset.miscs_idx_to_dbid)       # Number of misc. types (rows in the DB)\n",
    "    self.num_microorganism_types = len(dataset.mos_idx_to_dbid)         # Number of microrganism types (rows in the DB)\n",
    "    \n",
    "    self.num_mash_step_types  = len(dataset.mash_step_idx_to_name)  # Number of mash step types (e.g., Infusion, Decoction, Temperature)\n",
    "    self.num_hop_stage_types  = len(dataset.hop_stage_idx_to_name)  # Number of hop stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_misc_stage_types = len(dataset.misc_stage_idx_to_name) # Number of misc stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_mo_stage_types   = len(dataset.mo_stage_idx_to_name)   # Number of microorganism stage types (e.g., Primary, Secondary)\n",
    "    \n",
    "    # Embedding sizes ***\n",
    "    self.grain_type_embed_size         = GRAIN_TYPE_EMBED_SIZE\n",
    "    self.adjunct_type_embed_size       = ADJUNCT_TYPE_EMBED_SIZE\n",
    "    self.hop_type_embed_size           = HOP_TYPE_EMBED_SIZE\n",
    "    self.misc_type_embed_size          = MISC_TYPE_EMBED_SIZE\n",
    "    self.microorganism_type_embed_size = MICROORGANISM_TYPE_EMBED_SIZE\n",
    "    \n",
    "    # Network-specific hyperparameters/constraints ***\n",
    "    self.num_hidden_layers = 1\n",
    "    self.hidden_size = 2048\n",
    "    self.z_size = 128 # Latent-bottleneck dimension\n",
    "    self.activation_fn = nn.ELU\n",
    "    self.gain = nn.init.calculate_gain('linear', None) # Make sure this corresponds to the activation function!\n",
    "\n",
    "    # VAE-specific hyperparameters ***\n",
    "    self.beta_vae_gamma = 1000\n",
    "    self.max_beta_vae_capacity = 25\n",
    "    self.beta_vae_C_stop_iter = 1e5\n",
    "    \n",
    "    # Embedding labels ***\n",
    "    #grain_type_embedding_labels = \n",
    "    #adjunct_type_embedding_labels\n",
    "    #hop_type_embedding_labels\n",
    "    #misc_type_embedding_labels   \n",
    "    #microorganism_type_embedding_labels\n",
    "  \n",
    "  @property\n",
    "  def num_toplvl_inputs(self):\n",
    "    # (boil_time + mash_ph + sparge_temp)\n",
    "    return 3 \n",
    "  @property\n",
    "  def num_mash_step_inputs(self):\n",
    "     # Mash steps (step_type_index_size + step_time + step_temp) * (number of slots) - ordering assumed [0: step 1, 1: step 2, etc.]\n",
    "    return self.num_mash_steps*(self.num_mash_step_types + 2)\n",
    "  @property\n",
    "  def num_ferment_stage_inputs(self):\n",
    "    # Fermentation stages (step_time + step_temp) * (number of stages) - ordering assumed [0: primary, 1: secondary]\n",
    "    return self.num_ferment_stage_slots*(2)\n",
    "  @property\n",
    "  def num_grain_slot_inputs(self):\n",
    "    # Grain/Malt bill slots (grain_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_grain_slots*(self.grain_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_adjunct_slot_inputs(self):\n",
    "    # Adjunct slots (adjunct_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_adjunct_slots*(self.adjunct_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_hop_slot_inputs(self):\n",
    "    # Hop slots (hop_type_embed_size + stage_type_index_size + time + concentration) * (number of slots) - no ordering\n",
    "    return self.num_hop_slots*(self.hop_type_embed_size + self.num_hop_stage_types + 2)\n",
    "  @property\n",
    "  def num_misc_slot_inputs(self):\n",
    "    # Misc. slots (misc_type_embed_size + stage_type_index_size + time + amounts) * (number of slots) - no ordering\n",
    "    return self.num_misc_slots*(self.misc_type_embed_size + self.num_misc_stage_types + 2)\n",
    "  @property\n",
    "  def num_microorganism_slot_inputs(self):\n",
    "    # Microorganism slots (mo_type_embed_size + stage_type_index_size) * (number of slots) - no ordering\n",
    "    return self.num_microorganism_slots*(self.microorganism_type_embed_size + self.num_mo_stage_types)\n",
    "  \n",
    "  @property\n",
    "  def num_inputs(self):\n",
    "    \"\"\"Determine the number of inputs to the network.\n",
    "    Returns:\n",
    "        int: The total number of network inputs.\n",
    "    \"\"\"\n",
    "    return self.num_toplvl_inputs + self.num_mash_step_inputs + self.num_ferment_stage_inputs + \\\n",
    "           self.num_grain_slot_inputs + self.num_adjunct_slot_inputs + self.num_hop_slot_inputs + \\\n",
    "           self.num_misc_slot_inputs + self.num_microorganism_slot_inputs   \n",
    "\n",
    "args = RecipeNetArgs(dataset)\n",
    "args.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeNetData(object):\n",
    "  def __init__(self) -> None:\n",
    "    pass\n",
    "  \n",
    "class RecipeNetHeadEncoder(nn.Module):\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    # Embeddings (NOTE: Any categoricals that don't have embeddings will be one-hot encoded)\n",
    "    self.grain_type_embedding         = nn.Embedding(args.num_grain_types, args.grain_type_embed_size)\n",
    "    self.adjunct_type_embedding       = nn.Embedding(args.num_adjunct_types, args.adjunct_type_embed_size) \n",
    "    self.hop_type_embedding           = nn.Embedding(args.num_hop_types, args.hop_type_embed_size)\n",
    "    self.misc_type_embedding          = nn.Embedding(args.num_misc_types, args.misc_type_embed_size)\n",
    "    self.microorganism_type_embedding = nn.Embedding(args.num_microorganism_types, args.microorganism_type_embed_size)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x):\n",
    "    heads = RecipeNetData()\n",
    "    # Simple top-level heads (high-level recipe parameters)\n",
    "    heads.x_toplvl = torch.cat((x['boil_time'].unsqueeze(1), x['mash_ph'].unsqueeze(1), x['sparge_temp'].unsqueeze(1)), dim=1) # (B, 3)\n",
    "    \n",
    "    # Mash step heads\n",
    "    # NOTE: Data shape is (B, S=number_of_mash_steps) for the\n",
    "    # following recipe tensors: {'mash_step_type_inds', 'mash_step_times', 'mash_step_avg_temps'}\n",
    "    num_mash_step_types = self.args.num_mash_step_types\n",
    "    heads.enc_mash_step_type_onehot = F.one_hot(x['mash_step_type_inds'].long(), num_mash_step_types).float().flatten(1) # (B, S, num_mash_step_types) -> (B, S*num_mash_step_types) = [B, 24]\n",
    "    heads.x_mash_steps = torch.cat((heads.enc_mash_step_type_onehot, x['mash_step_times'], x['mash_step_avg_temps']), dim=1) # (B, num_mash_step_types*S+S+S) = [B, 36=(24+6+6)]\n",
    "    \n",
    "    # Ferment stage heads\n",
    "    # NOTE: Data shape is (B, S=2) for the following recipe tensors: {'ferment_stage_times', 'ferment_stage_temps'}\n",
    "    heads.x_ferment_stages = torch.cat((x['ferment_stage_times'], x['ferment_stage_temps']), dim=1) # (B, S+S)\n",
    "\n",
    "    # Grain (malt bill) heads\n",
    "    # NOTE: Data shape is (B, S=num_grain_slots) for the following recipe tensors: {'grain_core_type_inds', 'grain_amts'}\n",
    "    num_grain_types = self.args.num_grain_types\n",
    "    heads.enc_grain_type_embed = self.grain_type_embedding(x['grain_core_type_inds']).flatten(1) # (B, S, grain_type_embed_size) -> (B, S*grain_type_embed_size)\n",
    "    heads.enc_grain_type_onehot = F.one_hot(x['grain_core_type_inds'].long(), num_grain_types).float() # (B, num_grain_slots, num_grain_types)\n",
    "    heads.x_grains = torch.cat((heads.enc_grain_type_embed, x['grain_amts']), dim=1) # (B, S*grain_type_embed_size+S)\n",
    "    \n",
    "    # Adjunct heads\n",
    "    # NOTE: Data shape is (B, S=num_adjunct_slots) for the following recipe tensors: {'adjunct_core_type_inds', 'adjunct_amts'}\n",
    "    num_adjunct_types = self.args.num_adjunct_types\n",
    "    heads.enc_adjunct_type_embed = self.adjunct_type_embedding(x['adjunct_core_type_inds']).flatten(1) # (B, S, adjunct_type_embed_size) -> (B, S*adjunct_type_embed_size)\n",
    "    heads.enc_adjunct_type_onehot = F.one_hot(x['adjunct_core_type_inds'].long(), num_adjunct_types).float() # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    heads.x_adjuncts = torch.cat((heads.enc_adjunct_type_embed, x['adjunct_amts']), dim=1) # (B, S*adjunct_type_embed_size+S)\n",
    "    \n",
    "    # Hop heads\n",
    "    # NOTE: Data shape is (B, S=num_hop_slots) for the following recipe tensors: \n",
    "    # {'hop_type_inds', 'hop_stage_type_inds', 'hop_times', 'hop_concentrations'}\n",
    "    num_hop_types = self.args.num_hop_types\n",
    "    num_hop_stage_types = self.args.num_hop_stage_types\n",
    "    heads.enc_hop_type_embed = self.hop_type_embedding(x['hop_type_inds']).flatten(1) # (B, S, hop_type_embed_size)\n",
    "    heads.enc_hop_type_onehot = F.one_hot(x['hop_type_inds'].long(), num_hop_types).float() # (B, num_hop_slots, num_hop_types)\n",
    "    heads.enc_hop_stage_type_onehot = F.one_hot(x['hop_stage_type_inds'].long(), num_hop_stage_types).float().flatten(1) # (B, S, num_hop_stage_types)\n",
    "    heads.x_hops = torch.cat((heads.enc_hop_type_embed, heads.enc_hop_stage_type_onehot, x['hop_times'], x['hop_concentrations']), dim=1) # (B, S*hop_type_embed_size + S*num_hop_stage_types + S + S)\n",
    "    \n",
    "    # Misc. heads\n",
    "    # NOTE: Data shape is (B, S=num_misc_slots) for the following recipe tensors:\n",
    "    # {'misc_type_inds', 'misc_stage_inds', 'misc_times', 'misc_amts'}\n",
    "    num_misc_types = self.args.num_misc_types\n",
    "    num_misc_stage_types = self.args.num_misc_stage_types\n",
    "    heads.enc_misc_type_embed = self.misc_type_embedding(x['misc_type_inds']).flatten(1) # (B, S, misc_type_embed_size)\n",
    "    heads.enc_misc_type_onehot = F.one_hot(x['misc_type_inds'].long(), num_misc_types).float() # (B, num_misc_slots, num_misc_types)\n",
    "    heads.enc_misc_stage_type_onehot = F.one_hot(x['misc_stage_inds'].long(), num_misc_stage_types).float().flatten(1) # (B, S, num_misc_stage_types)\n",
    "    heads.x_miscs = torch.cat((heads.enc_misc_type_embed, heads.enc_misc_stage_type_onehot, x['misc_times'], x['misc_amts']), dim=1) # (B, S*misc_type_embed_size + S*num_misc_stage_types + S + S)\n",
    "    \n",
    "    # Microorganism heads\n",
    "    # NOTE: Data shape is (B, S=num_microorganism_slots) for the following recipe tensors:\n",
    "    # {'mo_type_inds', 'mo_stage_inds'}\n",
    "    num_mo_types = self.args.num_microorganism_types\n",
    "    num_mo_stage_types = self.args.num_mo_stage_types\n",
    "    heads.enc_mo_type_embed = self.microorganism_type_embedding(x['mo_type_inds']).flatten(1) # (B, S, microorganism_type_embed_size)\n",
    "    heads.enc_mo_type_onehot = F.one_hot(x['mo_type_inds'].long(), num_mo_types).float() # (B, num_mo_slots, num_mo_types)\n",
    "    heads.enc_mo_stage_type_onehot = F.one_hot(x['mo_stage_inds'].long(), num_mo_stage_types).float().flatten(1) # (B, S, num_mo_stage_types)\n",
    "    heads.x_mos = torch.cat((heads.enc_mo_type_embed, heads.enc_mo_stage_type_onehot), dim=1) # (B, S*microorganism_type_embed_size + S*num_mo_stage_types)\n",
    "    \n",
    "    # Put all the recipe data together into a flattened tensor\n",
    "    x = torch.cat((heads.x_toplvl, heads.x_mash_steps, heads.x_ferment_stages, heads.x_grains, heads.x_adjuncts, heads.x_hops, heads.x_miscs, heads.x_mos), dim=1) # (B, num_inputs)\n",
    "    return x, heads\n",
    "\n",
    "class RecipeNetFootDecoder(nn.Module):\n",
    "  def __init__(self, args: RecipeNetArgs) -> None:\n",
    "    super().__init__()\n",
    "    gain = args.gain\n",
    "    self.grain_type_decoder         = layer_init_xavier(nn.Linear(args.grain_type_embed_size, args.num_grain_types), gain)\n",
    "    self.adjunct_type_decoder       = layer_init_xavier(nn.Linear(args.adjunct_type_embed_size, args.num_adjunct_types), gain)\n",
    "    self.hop_type_decoder           = layer_init_xavier(nn.Linear(args.hop_type_embed_size, args.num_hop_types), gain)\n",
    "    self.misc_type_decoder          = layer_init_xavier(nn.Linear(args.misc_type_embed_size, args.num_misc_types), gain)\n",
    "    self.microorganism_type_decoder = layer_init_xavier(nn.Linear(args.microorganism_type_embed_size, args.num_microorganism_types), gain)\n",
    "    \n",
    "    # [Top-level recipe attributes, Mash steps, Fermentation stages, Grains, Adjuncts, Hops, Misc, Microorganisms]\n",
    "    self.split_sizes = [\n",
    "      args.num_toplvl_inputs, args.num_mash_step_inputs, args.num_ferment_stage_inputs, \n",
    "      args.num_grain_slot_inputs, args.num_adjunct_slot_inputs, args.num_hop_slot_inputs,\n",
    "      args.num_misc_slot_inputs, args.num_microorganism_slot_inputs\n",
    "    ]\n",
    "    #assert np.sum(se)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x_hat):\n",
    "    foots = RecipeNetData()\n",
    "    \n",
    "    # The decoded tensor is flat with a shape of (B, num_inputs), we'll need to break it apart\n",
    "    # so that we can eventually calculate losses appropriately for each head of original data fed to the encoder\n",
    "    foots.x_hat_toplvl, foots.x_hat_mash_steps, foots.x_hat_ferment_stages, foots.x_hat_grains, foots.x_hat_adjuncts, foots.x_hat_hops, foots.x_hat_miscs, foots.x_hat_mos = torch.split(x_hat, self.split_sizes, dim=1)\n",
    "\n",
    "    # Mash steps\n",
    "    num_mash_steps = self.args.num_mash_steps\n",
    "    enc_mash_step_type_onehot_size = num_mash_steps * self.args.num_mash_step_types\n",
    "    foots.dec_mash_step_type_onehot, foots.dec_mash_step_times, foots.dec_mash_step_avg_temps = torch.split(\n",
    "      foots.x_hat_mash_steps, [enc_mash_step_type_onehot_size, num_mash_steps, num_mash_steps], dim=1\n",
    "    )\n",
    "\n",
    "    # Grain slots\n",
    "    num_grain_slots = self.args.num_grain_slots\n",
    "    grain_type_embed_size = self.args.grain_type_embed_size\n",
    "    enc_grain_type_embed_size = num_grain_slots * grain_type_embed_size\n",
    "    foots.dec_grain_type_embed, foots.dec_grain_amts = torch.split(foots.x_hat_grains, [enc_grain_type_embed_size, num_grain_slots], dim=1)\n",
    "    foots.dec_grain_type_logits = self.grain_type_decoder(foots.dec_grain_type_embed.view(-1, num_grain_slots, grain_type_embed_size)) # (B, num_grain_slots, num_grain_types)\n",
    "\n",
    "    # Adjunct slots\n",
    "    num_adjunct_slots = self.args.num_adjunct_slots\n",
    "    adjunct_type_embed_size = self.args.adjunct_type_embed_size\n",
    "    enc_adjunct_type_embed_size = num_adjunct_slots * adjunct_type_embed_size\n",
    "    dec_adjunct_type_embed, foots.dec_adjunct_amts = torch.split(foots.x_hat_adjuncts, [enc_adjunct_type_embed_size, num_adjunct_slots], dim=1)\n",
    "    foots.dec_adjunct_type_logits = self.adjunct_type_decoder(dec_adjunct_type_embed.view(-1, num_adjunct_slots, adjunct_type_embed_size)) # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    \n",
    "    # Hop slots\n",
    "    num_hop_slots = self.args.num_hop_slots\n",
    "    hop_type_embed_size = self.args.hop_type_embed_size\n",
    "    enc_hop_type_embed_size = num_hop_slots * hop_type_embed_size\n",
    "    enc_hop_stage_type_onehot_size = num_hop_slots * self.args.num_hop_stage_types\n",
    "    dec_hop_type_embed, foots.dec_hop_stage_type_onehot, foots.dec_hop_times, foots.dec_hop_concentrations = torch.split(\n",
    "      foots.x_hat_hops, [enc_hop_type_embed_size, enc_hop_stage_type_onehot_size, num_hop_slots, num_hop_slots], dim=1\n",
    "    )\n",
    "    foots.dec_hop_type_logits = self.hop_type_decoder(dec_hop_type_embed.view(-1, num_hop_slots, hop_type_embed_size)) # (B, num_hop_slots, num_hop_types)\n",
    "    \n",
    "    # Miscellaneous slots\n",
    "    num_misc_slots = self.args.num_misc_slots\n",
    "    misc_type_embed_size = self.args.misc_type_embed_size\n",
    "    enc_misc_type_embed_size = num_misc_slots * misc_type_embed_size\n",
    "    enc_misc_stage_type_onehot_size = num_misc_slots * self.args.num_misc_stage_types\n",
    "    dec_misc_type_embed, foots.dec_misc_stage_type_onehot, foots.dec_misc_times, foots.dec_misc_amts = torch.split(\n",
    "      foots.x_hat_miscs, [enc_misc_type_embed_size, enc_misc_stage_type_onehot_size, num_misc_slots, num_misc_slots], dim=1\n",
    "    )\n",
    "    foots.dec_misc_type_logits = self.misc_type_decoder(dec_misc_type_embed.view(-1, num_misc_slots, misc_type_embed_size)) # (B, num_misc_slots, num_misc_types)\n",
    "    \n",
    "    # Microorganism slots\n",
    "    num_mo_slots = self.args.num_microorganism_slots\n",
    "    mo_type_embed_size = self.args.microorganism_type_embed_size\n",
    "    enc_mo_type_embed_size = num_mo_slots * mo_type_embed_size\n",
    "    enc_mo_stage_type_onehot_size = num_mo_slots * self.args.num_mo_stage_types\n",
    "    dec_mo_type_embed, foots.dec_mo_stage_type_onehot = torch.split(\n",
    "      foots.x_hat_mos, [enc_mo_type_embed_size, enc_mo_stage_type_onehot_size], dim=1\n",
    "    )\n",
    "    foots.dec_mo_type_logits = self.microorganism_type_decoder(dec_mo_type_embed.view(-1, num_mo_slots, mo_type_embed_size)) # (B, num_mo_slots, num_mo_types)\n",
    "    \n",
    "    return foots\n",
    "    \n",
    "\n",
    "class RecipeNet(nn.Module):\n",
    "\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    \n",
    "    hidden_size = args.hidden_size\n",
    "    z_size = args.z_size\n",
    "    activation_fn = args.activation_fn\n",
    "    gain = args.gain\n",
    "    \n",
    "    assert args.num_inputs >= 1\n",
    "    assert args.num_hidden_layers >= 1\n",
    "    assert hidden_size >= 1\n",
    "    assert z_size >= 1 and z_size < args.num_inputs\n",
    "\n",
    "    # Encoder and decoder networks\n",
    "    self.encoder = nn.Sequential()\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(args.num_inputs, hidden_size), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    for _ in range(1, args.num_hidden_layers):\n",
    "      self.encoder.append(layer_init_xavier(nn.Linear(hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(hidden_size, z_size*2), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    self.encoder.append(nn.BatchNorm1d(z_size*2))\n",
    "\n",
    "    self.decoder = nn.Sequential()\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(z_size, hidden_size), gain))\n",
    "    self.decoder.append(activation_fn())\n",
    "    for _ in range(1, args.num_hidden_layers):\n",
    "      self.decoder.append(layer_init_xavier(nn.Linear(hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(hidden_size, args.num_inputs), gain))\n",
    "    \n",
    "    # Pre-net Encoder (Network 'Heads')\n",
    "    self.head_encoder = RecipeNetHeadEncoder(args)\n",
    "    # Post-net Decoder (Network 'Foots')\n",
    "    self.foot_decoder = RecipeNetFootDecoder(args)\n",
    "\n",
    "    self.gamma = args.beta_vae_gamma\n",
    "    self.C_stop_iter = args.beta_vae_C_stop_iter\n",
    "    self.C_max = torch.Tensor([args.max_beta_vae_capacity])\n",
    "    \n",
    "    self.args = args\n",
    "  \n",
    "  def encode(self, input: torch.Tensor):\n",
    "    # Start by breaking the given x apart into all the various heads/embeddings \n",
    "    # and concatenate them into a value that can be fed to the encoder network\n",
    "    x, heads = self.head_encoder(input)\n",
    "    # Encode to the latent distribution mean and std dev.\n",
    "    mean, logvar = torch.chunk(self.encoder(x), 2, dim=-1) \n",
    "    return heads, mean, logvar\n",
    "  \n",
    "  def decode(self, z: torch.Tensor):\n",
    "    # Decode to the flattened output\n",
    "    x_hat = self.decoder(z)\n",
    "    # We need to perform the reverse process on the output from the decoder network:\n",
    "    # Break apart the output into matching segments similar to the heads (foots!) for use in later loss calculations\n",
    "    foots = self.foot_decoder(x_hat)\n",
    "    return foots\n",
    "    \n",
    "  def forward(self, input: torch.Tensor, use_mean=False):\n",
    "    heads, mean, logvar = self.encode(input)\n",
    "    # Sample (reparameterize trick) the final latent vector (z)\n",
    "    z = mean if use_mean else reparameterize(mean, logvar)\n",
    "    foots = self.decode(z)\n",
    "\n",
    "    return heads, foots, mean, logvar\n",
    "  \n",
    "  def loss_fn(self, input, heads, foots, mean, logvar, num_iter, kl_weight=1.0):\n",
    "    REDUCTION = 'sum'\n",
    "    # TODO: Simplify all this stuff into fewer losses: \n",
    "    # Group together all BCELogit and MSE losses into singluar tensors in both x and x_hat\n",
    "    loss_toplvl = F.mse_loss(foots.x_hat_toplvl, heads.x_toplvl, reduction=REDUCTION)\n",
    "    loss_mash_steps = F.binary_cross_entropy_with_logits(foots.dec_mash_step_type_onehot, heads.enc_mash_step_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_mash_step_times, input['mash_step_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_mash_step_avg_temps, input['mash_step_avg_temps'], reduction=REDUCTION)\n",
    "    loss_ferment_stages = F.mse_loss(foots.x_hat_ferment_stages, heads.x_ferment_stages, reduction=REDUCTION)\n",
    "    loss_grains = F.binary_cross_entropy_with_logits(foots.dec_grain_type_logits, heads.enc_grain_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_grain_amts, input['grain_amts'], reduction=REDUCTION)\n",
    "    loss_adjuncts = F.binary_cross_entropy_with_logits(foots.dec_adjunct_type_logits, heads.enc_adjunct_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_adjunct_amts, input['adjunct_amts'], reduction=REDUCTION)\n",
    "    loss_hops = F.binary_cross_entropy_with_logits(foots.dec_hop_type_logits, heads.enc_hop_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_hop_stage_type_onehot, heads.enc_hop_stage_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_hop_times, input['hop_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_hop_concentrations, input['hop_concentrations'], reduction=REDUCTION)\n",
    "    loss_miscs = F.binary_cross_entropy_with_logits(foots.dec_misc_type_logits, heads.enc_misc_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_misc_stage_type_onehot, heads.enc_misc_stage_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_misc_times, input['misc_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_misc_amts, input['misc_amts'], reduction=REDUCTION)\n",
    "    loss_mos = F.binary_cross_entropy_with_logits(foots.dec_mo_type_logits, heads.enc_mo_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_mo_stage_type_onehot, heads.enc_mo_stage_type_onehot, reduction=REDUCTION)\n",
    "\n",
    "    # Add up all our losses for reconstruction of the recipe\n",
    "    reconst_loss = loss_toplvl + loss_mash_steps + loss_ferment_stages + loss_grains + loss_adjuncts + loss_hops + loss_miscs + loss_mos\n",
    "\n",
    "    # Beta-VAE KL calculation is based on https://arxiv.org/pdf/1804.03599.pdf\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1), dim=0)\n",
    "    C = torch.clamp(self.C_max/self.C_stop_iter * num_iter, 0, self.C_max.data[0])\n",
    "    loss = reconst_loss + kl_weight * self.gamma * (kl_loss - C).abs()\n",
    "    return loss, C\n",
    "    \n",
    "  \n",
    "recipe_net = RecipeNet(args)\n",
    "optimizer  = torch.optim.Adam(recipe_net.parameters(), lr=1e-3, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3000, eps=1e-5)\n",
    "optimizer.param_groups[0]['lr'] = 1e-3 # Learning Rate\n",
    "global_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1259\u001b[0m, in \u001b[0;36mPyDB.apply_files_filter\u001b[0;34m(self, frame, original_filename, force_check_project_scope)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_filter_cache[cache_key]\n\u001b[1;32m   1260\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (623, '/Users/callumhay/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/data/dataloader.py', False, <code object __next__ at 0x7faae0695c60, file \"/Users/callumhay/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 623>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1167\u001b[0m, in \u001b[0;36mPyDB.in_project_scope\u001b[0;34m(self, frame, absolute_filename)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     cache_key \u001b[39m=\u001b[39m (frame\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_firstlineno, absolute_filename, frame\u001b[39m.\u001b[39mf_code)\n\u001b[0;32m-> 1167\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_in_project_scope_cache[cache_key]\n\u001b[1;32m   1168\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (623, '/Users/callumhay/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/data/dataloader.py', <code object __next__ at 0x7faae0695c60, file \"/Users/callumhay/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 623>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/callumhay/projects/brewbrain/network/network_test.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m   epoch_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m   \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     heads, foots, mean, logvar \u001b[39m=\u001b[39m recipe_net(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     loss, C \u001b[39m=\u001b[39m recipe_net\u001b[39m.\u001b[39mloss_fn(batch, heads, foots, mean, logvar, global_step, KL_WEIGHT)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/data/dataloader.py:623\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    624\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_name):\n\u001b[1;32m    625\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1712\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1284\u001b[0m, in \u001b[0;36mPyDB.apply_files_filter\u001b[0;34m(self, frame, original_filename, force_check_project_scope)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_filter_cache[cache_key] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1284\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_libraries_filter_enabled \u001b[39mor\u001b[39;00m force_check_project_scope) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_project_scope(frame):\n\u001b[1;32m   1285\u001b[0m     \u001b[39m# ignore library files while stepping\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_filter_cache[cache_key] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[39mif\u001b[39;00m force_check_project_scope:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1191\u001b[0m, in \u001b[0;36mPyDB.in_project_scope\u001b[0;34m(self, frame, absolute_filename)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     cache[cache_key] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1191\u001b[0m     cache[cache_key] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_files_filtering\u001b[39m.\u001b[39;49min_project_roots(absolute_filename)\n\u001b[1;32m   1193\u001b[0m \u001b[39mreturn\u001b[39;00m cache[cache_key]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_filtering.py:260\u001b[0m, in \u001b[0;36mFilesFiltering.in_project_roots\u001b[0;34m(self, received_filename)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    258\u001b[0m project_roots \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_project_roots()  \u001b[39m# roots are absolute/normalized.\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m absolute_normalized_filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_absolute_normalized_path(received_filename)\n\u001b[1;32m    261\u001b[0m absolute_normalized_filename_as_dir \u001b[39m=\u001b[39m absolute_normalized_filename \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m IS_WINDOWS \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m found_in_project \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_filtering.py:224\u001b[0m, in \u001b[0;36mFilesFiltering._absolute_normalized_path\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_absolute_normalized_path\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[1;32m    221\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m    Provides a version of the filename that's absolute and normalized.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m normcase(pydevd_file_utils\u001b[39m.\u001b[39;49mabsolute_path(filename))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:373\u001b[0m, in \u001b[0;36mabsolute_path\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    This returns a filename that is canonical and it's meant to be used internally\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    to store information on breakpoints and see if there's any hit on it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39m    in the editor).\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m get_abs_path_real_path_and_base_from_file(filename)[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mabsolute_path\u001b[39m(filename):\n\u001b[1;32m    374\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39m    Provides a version of the filename that's absolute (and NOT normalized).\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39mreturn\u001b[39;00m get_abs_path_real_path_and_base_from_file(filename)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "run_dir = os.path.join(\"runs\", f\"recipe_vae_{int(time.time())}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "writer = SummaryWriter(run_dir)\n",
    "writer.add_text(\n",
    "  \"hyperparameters\",\n",
    "  \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()]))\n",
    ")\n",
    "\n",
    "# Monitor the recipe network using hooks and tensorboard\n",
    "for name, layer in recipe_net.named_children():\n",
    "  if name == 'encoder':\n",
    "    encoder_children = list(layer.named_children())\n",
    "    # Distribution after the first layer+activation\n",
    "    first_actfn = encoder_children[1][1]\n",
    "    first_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/encoder_first_actfn\", output.flatten(), global_step, bins=100) if global_step % 10 == 0 else None\n",
    "    )\n",
    "    # Distribution after the last layer+activation (before batchnorm)\n",
    "    last_actfn = encoder_children[1+args.num_hidden_layers*2][1]\n",
    "    last_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/encoder_last_actfn\", output.flatten(), global_step, bins='auto') if global_step % 10 == 0 else None\n",
    "    )\n",
    "    # Distribution after the encoder (last layer is a batchnorm1D)\n",
    "    batchnorm = encoder_children[2+args.num_hidden_layers*2][1]\n",
    "    batchnorm.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/encoder_batchnorm\", output.flatten(), global_step, bins='auto') if global_step % 10 == 0 else None\n",
    "    )\n",
    "    \n",
    "  elif name == 'decoder':\n",
    "    pass\n",
    "  elif name == 'head_encoder':\n",
    "    # Send the head encoder's embeddings to tensorboard\n",
    "    for embed_name, embed in layer.named_children():\n",
    "      embed.register_forward_hook(\n",
    "        lambda layer, input, output:\n",
    "          writer.add_embedding(layer.weight, )\n",
    "      )\n",
    "    pass\n",
    "  else: # name == 'foot_decoder':\n",
    "    pass\n",
    "\n",
    "KL_WEIGHT  = 1.0\n",
    "NUM_EPOCHS = 10\n",
    "outlier_ids = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "  epoch_loss = 0.0\n",
    "  for batch_idx, batch in enumerate(dataloader):\n",
    "    heads, foots, mean, logvar = recipe_net(batch)\n",
    "    loss, C = recipe_net.loss_fn(batch, heads, foots, mean, logvar, global_step, KL_WEIGHT)\n",
    "    \n",
    "    epoch_loss += loss.item()\n",
    "    writer.add_scalar(\"charts/total_loss\", loss.item(), global_step)\n",
    "    #if loss.item() > 3.5e4:\n",
    "    #  print(batch['dbid'])\n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(recipe_net.parameters(), 100.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    global_step += 1\n",
    "    \n",
    "    print('\\r', \"Global Step:\", global_step, \"Loss:\", np.around(loss.item(), 5), \"lr:\", optimizer.param_groups[0]['lr'], \"C:\", np.around(C.item(), 1), \"\\t\\t\", end='')\n",
    "  \n",
    "  avg_epoch_loss = epoch_loss / (batch_idx+1)\n",
    "  print(\"\\r\\n\", f\"Avg Epoch #{i+1} loss: {np.around(avg_epoch_loss,5)}\\t\\t\\t\\t\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('brewbrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf149ef9249c8031bd8b677d42c7b958c45095f85a0c42a709c15a6e71aa8835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
