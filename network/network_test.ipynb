{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from recipedataset import RecipeDataset, RECIPE_DATASET_FILENAME, NUM_GRAIN_SLOTS, NUM_ADJUNCT_SLOTS, NUM_HOP_SLOTS, NUM_MISC_SLOTS, NUM_MICROORGANISM_SLOTS, NUM_FERMENT_STAGE_SLOTS, NUM_MASH_STEPS\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def layer_init_ortho(layer, std=np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, std)\n",
    "  nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def layer_init_xavier(layer, gain):\n",
    "  nn.init.xavier_normal_(layer.weight, gain)\n",
    "  nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "  std = torch.exp(0.5 * logvar)\n",
    "  eps = torch.randn_like(std)\n",
    "  return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Load the dataset and create a dataloader for it\n",
    "with open(\"../\" + RECIPE_DATASET_FILENAME, 'rb') as f:\n",
    "  dataset = pickle.load(f)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAIN_TYPE_EMBED_SIZE         = 32\n",
    "ADJUNCT_TYPE_EMBED_SIZE       = 32\n",
    "HOP_TYPE_EMBED_SIZE           = 96\n",
    "MISC_TYPE_EMBED_SIZE          = 64\n",
    "MICROORGANISM_TYPE_EMBED_SIZE = 64\n",
    "\n",
    "class RecipeNetArgs:\n",
    "  def __init__(self, dataset) -> None:\n",
    "    # Recipe-specific constraints ***\n",
    "    self.num_mash_steps          = NUM_MASH_STEPS\n",
    "    self.num_grain_slots         = NUM_GRAIN_SLOTS\n",
    "    self.num_adjunct_slots       = NUM_ADJUNCT_SLOTS\n",
    "    self.num_hop_slots           = NUM_HOP_SLOTS\n",
    "    self.num_misc_slots          = NUM_MISC_SLOTS\n",
    "    self.num_microorganism_slots = NUM_MICROORGANISM_SLOTS\n",
    "    self.num_ferment_stage_slots = NUM_FERMENT_STAGE_SLOTS\n",
    "    \n",
    "    # NOTE: All types include a \"None\" (i.e., empty) category\n",
    "    self.num_grain_types         = len(dataset.core_grains_idx_to_dbid) # Number of (core) grain types (rows in the DB)\n",
    "    self.num_adjunct_types       = len(dataset.core_adjs_idx_to_dbid)   # Number of (core) adjunct types (rows in the DB)\n",
    "    self.num_hop_types           = len(dataset.hops_idx_to_dbid)        # Number of hop types (rows in the DB)\n",
    "    self.num_misc_types          = len(dataset.miscs_idx_to_dbid)       # Number of misc. types (rows in the DB)\n",
    "    self.num_microorganism_types = len(dataset.mos_idx_to_dbid)         # Number of microrganism types (rows in the DB)\n",
    "    \n",
    "    self.num_mash_step_types  = len(dataset.mash_step_idx_to_name)  # Number of mash step types (e.g., Infusion, Decoction, Temperature)\n",
    "    self.num_hop_stage_types  = len(dataset.hop_stage_idx_to_name)  # Number of hop stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_misc_stage_types = len(dataset.misc_stage_idx_to_name) # Number of misc stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_mo_stage_types   = len(dataset.mo_stage_idx_to_name)   # Number of microorganism stage types (e.g., Primary, Secondary)\n",
    "    \n",
    "    # Embedding sizes ***\n",
    "    self.grain_type_embed_size         = GRAIN_TYPE_EMBED_SIZE\n",
    "    self.adjunct_type_embed_size       = ADJUNCT_TYPE_EMBED_SIZE\n",
    "    self.hop_type_embed_size           = HOP_TYPE_EMBED_SIZE\n",
    "    self.misc_type_embed_size          = MISC_TYPE_EMBED_SIZE\n",
    "    self.microorganism_type_embed_size = MICROORGANISM_TYPE_EMBED_SIZE\n",
    "    \n",
    "    # Network-specific hyperparameters/constraints ***\n",
    "    self.num_hidden_layers = 1\n",
    "    self.hidden_size = 1024\n",
    "    self.z_size = 64 # Latent-bottleneck dimension\n",
    "    self.activation_fn = nn.ELU\n",
    "    self.gain = nn.init.calculate_gain('linear', None) # Make sure this corresponds to the activation function!\n",
    "    self.num_inputs = self.calc_num_inputs()\n",
    "    \n",
    "    # VAE-specific hyperparameters\n",
    "    self.beta_vae_gamma = 1000\n",
    "    self.max_beta_vae_capacity = 25\n",
    "    self.beta_vae_C_stop_iter = 1e5\n",
    "    \n",
    "  \n",
    "  def calc_num_inputs(self):\n",
    "    \"\"\"Determine the number of inputs to the network.\n",
    "    Returns:\n",
    "        int: The total number of network inputs.\n",
    "    \"\"\"\n",
    "    # (boil_time + mash_ph + sparge_temp)\n",
    "    num_simple_inputs = 3 \n",
    "    # Mash steps (step_type_index_size + step_time + step_temp) * (number of slots) - ordering assumed [0: step 1, 1: step 2, etc.]\n",
    "    num_mash_step_inputs = self.num_mash_steps*(self.num_mash_step_types + 2)\n",
    "    # Fermentation stages (step_time + step_temp) * (number of stages) - ordering assumed [0: primary, 1: secondary]\n",
    "    num_ferment_step_inputs = self.num_ferment_stage_slots*(2)\n",
    "    # Grain/Malt bill slots (grain_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    num_grain_slot_inputs = self.num_grain_slots*(self.grain_type_embed_size + 1)\n",
    "    # Adjunct slots (adjunct_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    num_adjunct_slot_inputs = self.num_adjunct_slots*(self.adjunct_type_embed_size + 1)\n",
    "    # Hop slots (hop_type_embed_size + stage_type_index_size + time + concentration) * (number of slots) - no ordering\n",
    "    num_hop_slot_inputs = self.num_hop_slots*(self.hop_type_embed_size + self.num_hop_stage_types + 2)\n",
    "    # Misc. slots (misc_type_embed_size + stage_type_index_size + time + amounts) * (number of slots) - no ordering\n",
    "    num_misc_slot_inputs = self.num_misc_slots*(self.misc_type_embed_size + self.num_misc_stage_types + 2)\n",
    "    # Microorganism slots (mo_type_embed_size + stage_type_index_size) * (number of slots) - no ordering\n",
    "    num_mo_slot_inputs = self.num_microorganism_slots*(self.microorganism_type_embed_size + self.num_mo_stage_types)\n",
    "\n",
    "    return num_simple_inputs + num_mash_step_inputs + num_ferment_step_inputs + num_grain_slot_inputs + \\\n",
    "      num_adjunct_slot_inputs + num_hop_slot_inputs + num_misc_slot_inputs + num_mo_slot_inputs   \n",
    "\n",
    "args = RecipeNetArgs(dataset)\n",
    "args.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeVAE(nn.Module):\n",
    "\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    \n",
    "    hidden_size = args.hidden_size\n",
    "    z_size = args.z_size\n",
    "    activation_fn = args.activation_fn\n",
    "    gain = args.gain\n",
    "    \n",
    "    assert args.num_inputs >= 1\n",
    "    assert args.num_hidden_layers >= 1\n",
    "    assert hidden_size >= 1\n",
    "    assert z_size >= 1 and z_size < args.num_inputs\n",
    "\n",
    "    # Encoder and decoder networks\n",
    "    self.encoder = nn.Sequential()\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(args.num_inputs, hidden_size), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    for _ in range(1, args.num_hidden_layers):\n",
    "      self.encoder.append(layer_init_xavier(nn.Linear(hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(hidden_size, z_size*2), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    self.encoder.append(nn.BatchNorm1d(z_size*2))\n",
    "\n",
    "    self.decoder = nn.Sequential()\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(z_size, hidden_size), gain))\n",
    "    self.decoder.append(activation_fn())\n",
    "    for _ in range(1, args.num_hidden_layers):\n",
    "      self.decoder.append(layer_init_xavier(nn.Linear(hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(hidden_size, args.num_inputs), gain))\n",
    "    \n",
    "    # Embeddings (NOTE: Any categoricals that don't have embeddings will be one-hot encoded)\n",
    "    self.grain_type_embedding         = nn.Embedding(args.num_grain_types, args.grain_type_embed_size)\n",
    "    self.adjunct_type_embedding       = nn.Embedding(args.num_adjunct_types, args.adjunct_type_embed_size) \n",
    "    self.hop_type_embedding           = nn.Embedding(args.num_hop_types, args.hop_type_embed_size)\n",
    "    self.misc_type_embedding          = nn.Embedding(args.num_misc_types, args.misc_type_embed_size)\n",
    "    self.microorganism_type_embedding = nn.Embedding(args.num_microorganism_types, args.microorganism_type_embed_size)\n",
    "    \n",
    "    # Post-network decoders (these are basically learned inverse embeddings)\n",
    "    self.grain_type_decoder         = layer_init_xavier(nn.Linear(args.grain_type_embed_size, args.num_grain_types), gain)\n",
    "    self.adjunct_type_decoder       = layer_init_xavier(nn.Linear(args.adjunct_type_embed_size, args.num_adjunct_types), gain)\n",
    "    self.hop_type_decoder           = layer_init_xavier(nn.Linear(args.hop_type_embed_size, args.num_hop_types), gain)\n",
    "    self.misc_type_decoder          = layer_init_xavier(nn.Linear(args.misc_type_embed_size, args.num_misc_types), gain)\n",
    "    self.microorganism_type_decoder = layer_init_xavier(nn.Linear(args.microorganism_type_embed_size, args.num_microorganism_types), gain)\n",
    "    \n",
    "    self.gamma = args.beta_vae_gamma\n",
    "    self.C_stop_iter = args.beta_vae_C_stop_iter\n",
    "    self.C_max = torch.Tensor([args.max_beta_vae_capacity])\n",
    "    \n",
    "    self.args = args\n",
    "  \n",
    "  def forward(self, x, use_mean=False):\n",
    "    # Simple top-level heads (high-level recipe parameters)\n",
    "    x_toplvl = torch.cat((x['boil_time'].unsqueeze(1), x['mash_ph'].unsqueeze(1), x['sparge_temp'].unsqueeze(1)), dim=1) # (B, 3)\n",
    "    \n",
    "    # Mash step heads\n",
    "    # NOTE: Data shape is (B, S=number_of_mash_step_slots) for the\n",
    "    # following recipe tensors: {'mash_step_type_inds', 'mash_step_times', 'mash_step_avg_temps'}\n",
    "    num_mash_step_types = self.args.num_mash_step_types\n",
    "    enc_mash_step_type_onehot = F.one_hot(x['mash_step_type_inds'].long(), num_mash_step_types).float().flatten(1) # (B, S, num_mash_step_types) -> (B, S*num_mash_step_types) = [B, 24]\n",
    "    x_mash_steps = torch.cat((enc_mash_step_type_onehot, x['mash_step_times'], x['mash_step_avg_temps']), dim=1) # (B, num_mash_step_types*S+S+S) = [B, 36=(24+6+6)]\n",
    "    \n",
    "    # Ferment stage heads\n",
    "    # NOTE: Data shape is (B, S=2) for the following recipe tensors: {'ferment_stage_times', 'ferment_stage_temps'}\n",
    "    x_ferment_stages = torch.cat((x['ferment_stage_times'], x['ferment_stage_temps']), dim=1) # (B, S+S)\n",
    "\n",
    "    # Grain (malt bill) heads\n",
    "    # NOTE: Data shape is (B, S=num_grain_slots) for the following recipe tensors: {'grain_core_type_inds', 'grain_amts'}\n",
    "    enc_grain_type_embed = self.grain_type_embedding(x['grain_core_type_inds']).flatten(1) # (B, S, grain_type_embed_size) -> (B, S*grain_type_embed_size)\n",
    "    x_grains = torch.cat((enc_grain_type_embed, x['grain_amts']), dim=1) # (B, S*grain_type_embed_size+S)\n",
    "    \n",
    "    # Adjunct heads\n",
    "    # NOTE: Data shape is (B, S=num_adjunct_slots) for the following recipe tensors: {'adjunct_core_type_inds', 'adjunct_amts'}\n",
    "    enc_adjunct_type_embed = self.adjunct_type_embedding(x['adjunct_core_type_inds']).flatten(1) # (B, S, adjunct_type_embed_size) -> (B, S*adjunct_type_embed_size)\n",
    "    x_adjuncts = torch.cat((enc_adjunct_type_embed, x['adjunct_amts']), dim=1) # (B, S*adjunct_type_embed_size+S)\n",
    "    \n",
    "    # Hop heads\n",
    "    # NOTE: Data shape is (B, S=num_hop_slots) for the following recipe tensors: \n",
    "    # {'hop_type_inds', 'hop_stage_type_inds', 'hop_times', 'hop_concentrations'}\n",
    "    num_hop_stage_types = self.args.num_hop_stage_types\n",
    "    enc_hop_type_embed = self.hop_type_embedding(x['hop_type_inds']).flatten(1) # (B, S, hop_type_embed_size)\n",
    "    enc_hop_stage_type_onehot = F.one_hot(x['hop_stage_type_inds'].long(), num_hop_stage_types).float().flatten(1) # (B, S, num_hop_stage_types)\n",
    "    x_hops = torch.cat((enc_hop_type_embed, enc_hop_stage_type_onehot, x['hop_times'], x['hop_concentrations']), dim=1) # (B, S*hop_type_embed_size + S*num_hop_stage_types + S + S)\n",
    "    \n",
    "    # Misc. heads\n",
    "    # NOTE: Data shape is (B, S=num_misc_slots) for the following recipe tensors:\n",
    "    # {'misc_type_inds', 'misc_stage_inds', 'misc_times', 'misc_amts'}\n",
    "    num_misc_stage_types = self.args.num_misc_stage_types\n",
    "    enc_misc_type_embed = self.misc_type_embedding(x['misc_type_inds']).flatten(1) # (B, S, misc_type_embed_size)\n",
    "    enc_misc_stage_type_onehot = F.one_hot(x['misc_stage_inds'].long(), num_misc_stage_types).float().flatten(1) # (B, S, num_misc_stage_types)\n",
    "    x_miscs = torch.cat((enc_misc_type_embed, enc_misc_stage_type_onehot, x['misc_times'], x['misc_amts']), dim=1) # (B, S*misc_type_embed_size + S*num_misc_stage_types + S + S)\n",
    "    \n",
    "    # Microorganism heads\n",
    "    # NOTE: Data shape is (B, S=num_microorganism_slots) for the following recipe tensors:\n",
    "    # {'mo_type_inds', 'mo_stage_inds'}\n",
    "    num_mo_stage_types = self.args.num_mo_stage_types\n",
    "    enc_mo_type_embed = self.microorganism_type_embedding(x['mo_type_inds']).flatten(1) # (B, S, microorganism_type_embed_size)\n",
    "    enc_mo_stage_type_onehot = F.one_hot(x['mo_stage_inds'].long(), num_mo_stage_types).float().flatten(1) # (B, S, num_mo_stage_types)\n",
    "    x_mos = torch.cat((enc_mo_type_embed, enc_mo_stage_type_onehot), dim=1) # (B, S*microorganism_type_embed_size + S*num_mo_stage_types)\n",
    "    \n",
    "    # Put all the recipe data together into a flattened tensor\n",
    "    x_recipe = x\n",
    "    x = torch.cat((x_toplvl, x_mash_steps, x_ferment_stages, x_grains, x_adjuncts, x_hops, x_miscs, x_mos), dim=1) # (B, num_inputs)\n",
    "    \n",
    "    # Encode to the latent distribution, sample (reparameterize trick) then decode\n",
    "    mean, logvar = torch.chunk(self.encoder(x), 2, dim=-1) \n",
    "    z = mean if use_mean else reparameterize(mean, logvar)\n",
    "    x_hat = self.decoder(z)\n",
    "    \n",
    "    # TODO: Move the loss calculations into its own function...\n",
    "    # Maybe use a class/dict for holding intermediate values during the pipeline for easy passing between functions?\n",
    "    \n",
    "    # The decoded tensor is flat with a shape of (B, num_inputs), we'll need to break it apart to calculate the appropriate losses\n",
    "    x_hat_toplvl, x_hat_mash_steps, x_hat_ferment_stages, x_hat_grains, x_hat_adjuncts, x_hat_hops, x_hat_miscs, x_hat_mos = torch.split(\n",
    "      x_hat, [3, x_mash_steps.shape[1], x_ferment_stages.shape[1], x_grains.shape[1], x_adjuncts.shape[1], x_hops.shape[1], x_miscs.shape[1], x_mos.shape[1]], dim=1\n",
    "    )\n",
    "    \n",
    "    # TODO: Simplify all this stuff into fewer losses: \n",
    "    # Group together all BCELogit and MSE losses into singluar tensors in both x and x_hat\n",
    "    loss_toplvl = F.mse_loss(x_hat_toplvl, x_toplvl, reduction='sum')\n",
    "    \n",
    "    # Mash step decode and loss calculation\n",
    "    num_mash_steps = self.args.num_mash_steps\n",
    "    dec_mash_step_type_onehot, dec_mash_step_times, dec_mash_step_avg_temps = torch.split(\n",
    "      x_hat_mash_steps, [enc_mash_step_type_onehot.shape[1], num_mash_steps, num_mash_steps], dim=1\n",
    "    )\n",
    "    loss_mash_steps = F.binary_cross_entropy_with_logits(dec_mash_step_type_onehot, enc_mash_step_type_onehot, reduction='sum') + \\\n",
    "      F.mse_loss(dec_mash_step_times, x_recipe['mash_step_times'], reduction='sum') + F.mse_loss(dec_mash_step_avg_temps, x_recipe['mash_step_avg_temps'], reduction='sum')\n",
    "    \n",
    "    # Ferment stages loss calculation\n",
    "    loss_ferment_stages = F.mse_loss(x_hat_ferment_stages, x_ferment_stages, reduction='sum')\n",
    "    \n",
    "    # Grain slots decode and loss calculation\n",
    "    num_grain_slots = self.args.num_grain_slots\n",
    "    num_grain_types = self.args.num_grain_types\n",
    "    grain_type_embed_size = self.args.grain_type_embed_size\n",
    "    dec_grain_type_embed, dec_grain_amts = torch.split(x_hat_grains, [enc_grain_type_embed.shape[1], num_grain_slots], dim=1)\n",
    "    dec_grain_type_logits = self.grain_type_decoder(dec_grain_type_embed.view(-1, num_grain_slots, grain_type_embed_size)) # (B, num_grain_slots, num_grain_types)\n",
    "    enc_grain_type_onehot = F.one_hot(x_recipe['grain_core_type_inds'].long(), num_grain_types).float() # (B, num_grain_slots, num_grain_types)\n",
    "    loss_grains = F.binary_cross_entropy_with_logits(dec_grain_type_logits, enc_grain_type_onehot, reduction='sum') + F.mse_loss(dec_grain_amts, x_recipe['grain_amts'], reduction='sum')\n",
    "    \n",
    "    # Adjunct slots decode and loss calculation\n",
    "    num_adjunct_slots = self.args.num_adjunct_slots\n",
    "    num_adjunct_types = self.args.num_adjunct_types\n",
    "    adjunct_type_embed_size = self.args.adjunct_type_embed_size\n",
    "    dec_adjunct_type_embed, dec_adjunct_amts = torch.split(x_hat_adjuncts, [enc_adjunct_type_embed.shape[1], num_adjunct_slots], dim=1)\n",
    "    dec_adjunct_type_logits = self.adjunct_type_decoder(dec_adjunct_type_embed.view(-1, num_adjunct_slots, adjunct_type_embed_size)) # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    enc_grain_type_onehot = F.one_hot(x_recipe['adjunct_core_type_inds'].long(), num_adjunct_types).float() # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    loss_adjuncts = F.binary_cross_entropy_with_logits(dec_adjunct_type_logits, enc_grain_type_onehot, reduction='sum') + F.mse_loss(dec_adjunct_amts, x_recipe['adjunct_amts'], reduction='sum')\n",
    "    \n",
    "    # Hop slots decode and loss calculation\n",
    "    num_hop_slots = self.args.num_hop_slots\n",
    "    num_hop_types = self.args.num_hop_types\n",
    "    hop_type_embed_size = self.args.hop_type_embed_size\n",
    "    dec_hop_type_embed, dec_hop_stage_type_onehot, dec_hop_times, dec_hop_concentrations = torch.split(\n",
    "      x_hat_hops, [enc_hop_type_embed.shape[1], enc_hop_stage_type_onehot.shape[1], num_hop_slots, num_hop_slots], dim=1\n",
    "    )\n",
    "    dec_hop_type_logits = self.hop_type_decoder(dec_hop_type_embed.view(-1, num_hop_slots, hop_type_embed_size)) # (B, num_hop_slots, num_hop_types)\n",
    "    enc_hop_type_onehot = F.one_hot(x_recipe['hop_type_inds'].long(), num_hop_types).float() # (B, num_hop_slots, num_hop_types)\n",
    "    loss_hops = F.binary_cross_entropy_with_logits(dec_hop_type_logits, enc_hop_type_onehot, reduction='sum') + \\\n",
    "      F.binary_cross_entropy_with_logits(dec_hop_stage_type_onehot, enc_hop_stage_type_onehot, reduction='sum') + \\\n",
    "      F.mse_loss(dec_hop_times, x_recipe['hop_times'], reduction='sum') + \\\n",
    "      F.mse_loss(dec_hop_concentrations, x_recipe['hop_concentrations'], reduction='sum')\n",
    "    \n",
    "    # Miscellaneous slots decode and loss calculation\n",
    "    num_misc_slots = self.args.num_misc_slots\n",
    "    num_misc_types = self.args.num_misc_types\n",
    "    misc_type_embed_size = self.args.misc_type_embed_size\n",
    "    dec_misc_type_embed, dec_misc_stage_type_onehot, dec_misc_times, dec_misc_amts = torch.split(\n",
    "      x_hat_miscs, [enc_misc_type_embed.shape[1], enc_misc_stage_type_onehot.shape[1], num_misc_slots, num_misc_slots], dim=1\n",
    "    )\n",
    "    dec_misc_type_logits = self.misc_type_decoder(dec_misc_type_embed.view(-1, num_misc_slots, misc_type_embed_size)) # (B, num_misc_slots, num_misc_types)\n",
    "    enc_misc_type_onehot = F.one_hot(x_recipe['misc_type_inds'].long(), num_misc_types).float() # (B, num_misc_slots, num_misc_types)\n",
    "    loss_miscs = F.binary_cross_entropy_with_logits(dec_misc_type_logits, enc_misc_type_onehot, reduction='sum') + \\\n",
    "      F.binary_cross_entropy_with_logits(dec_misc_stage_type_onehot, enc_misc_stage_type_onehot, reduction='sum') + \\\n",
    "      F.mse_loss(dec_misc_times, x_recipe['misc_times'], reduction='sum') + \\\n",
    "      F.mse_loss(dec_misc_amts, x_recipe['misc_amts'], reduction='sum')\n",
    "    \n",
    "    # Microorganism slots decode and loss calculation\n",
    "    num_mo_slots = self.args.num_microorganism_slots\n",
    "    num_mo_types = self.args.num_microorganism_types\n",
    "    mo_type_embed_size = self.args.microorganism_type_embed_size\n",
    "    dec_mo_type_embed, dec_mo_stage_type_onehot = torch.split(\n",
    "      x_hat_mos, [enc_mo_type_embed.shape[1], enc_mo_stage_type_onehot.shape[1]], dim=1\n",
    "    )\n",
    "    dec_mo_type_logits = self.microorganism_type_decoder(dec_mo_type_embed.view(-1, num_mo_slots, mo_type_embed_size)) # (B, num_mo_slots, num_mo_types)\n",
    "    enc_mo_type_onehot = F.one_hot(x_recipe['mo_type_inds'].long(), num_mo_types).float() # (B, num_mo_slots, num_mo_types)\n",
    "    loss_mos = F.binary_cross_entropy_with_logits(dec_mo_type_logits, enc_mo_type_onehot, reduction='sum') + \\\n",
    "      F.binary_cross_entropy_with_logits(dec_mo_stage_type_onehot, enc_mo_stage_type_onehot, reduction='sum')\n",
    "    \n",
    "    # Add up all our losses for reconstruction of the recipe\n",
    "    reconst_loss = loss_toplvl + loss_mash_steps + loss_ferment_stages + loss_grains + loss_hops + loss_miscs + loss_mos\n",
    "    \n",
    "    \n",
    "    #return x_hat, mean, logvar\n",
    "    \n",
    "    \n",
    "  '''\n",
    "  def calc_loss(self, x, x_hat, mean, logvar, num_iter, kl_weight=1.0):\n",
    "    \n",
    "    #one_hot_types = nn.functional.one_hot(x.long(), NUM_CORE_GRAIN_TYPES).float()\n",
    "    #recons_loss = nn.functional.binary_cross_entropy_with_logits(x_hat, one_hot_types, reduction='sum')\n",
    "    \n",
    "    # Beta-VAE KL calculation is based on https://arxiv.org/pdf/1804.03599.pdf\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1), dim=0)\n",
    "    C = torch.clamp(self.C_max/self.C_stop_iter * num_iter, 0, self.C_max.data[0])\n",
    "    loss = recons_loss + kl_weight * self.gamma * (kl_loss - C).abs()\n",
    "    return loss\n",
    "  '''\n",
    "  \n",
    "recipe_net = RecipeVAE(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/callumhay/projects/brewbrain/network/network_test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   \u001b[39mif\u001b[39;00m batch_idx \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     recipe_net(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/callumhay/projects/brewbrain/network/network_test.ipynb Cell 5\u001b[0m in \u001b[0;36mRecipeVAE.forward\u001b[0;34m(self, x, use_mean)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m dec_misc_type_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmisc_type_decoder(dec_misc_type_embed\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_misc_slots, misc_type_embed_size)) \u001b[39m# (B, num_misc_slots, num_misc_types)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m enc_misc_type_onehot \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(x_recipe[\u001b[39m'\u001b[39m\u001b[39mmisc_type_inds\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlong(), num_misc_types) \u001b[39m# (B, num_misc_slots, num_misc_types)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m loss_miscs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(dec_misc_type_logits, enc_misc_type_onehot, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m   F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(dec_misc_stage_type_onehot, enc_misc_stage_type_onehot, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m   F\u001b[39m.\u001b[39mmse_loss(dec_misc_times, x_recipe[\u001b[39m'\u001b[39m\u001b[39mmisc_times\u001b[39m\u001b[39m'\u001b[39m], reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/callumhay/projects/brewbrain/network/network_test.ipynb#X15sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m   F\u001b[39m.\u001b[39mmse_loss(dec_misc_amts, x_recipe[\u001b[39m'\u001b[39m\u001b[39mmisc_amts\u001b[39m\u001b[39m'\u001b[39m], reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/functional.py:3162\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m-> 3162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "  if batch_idx == 1:\n",
    "    recipe_net(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('brewbrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf149ef9249c8031bd8b677d42c7b958c45095f85a0c42a709c15a6e71aa8835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
