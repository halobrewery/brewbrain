{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from brewbrain_db import Base, BREWBRAIN_DB_FILENAME, build_db_str\n",
    "from file_utils import find_file_cwd_and_parent_dirs\n",
    "from recipe_dataset import core_grain_labels, core_adjunct_labels, hop_labels, misc_labels, microorganism_labels\n",
    "from recipe_dataset import RecipeDataset, RECIPE_DATASET_FILENAME, NUM_GRAIN_SLOTS, NUM_ADJUNCT_SLOTS, NUM_HOP_SLOTS, NUM_MISC_SLOTS, NUM_MICROORGANISM_SLOTS, NUM_FERMENT_STAGE_SLOTS, NUM_MASH_STEPS\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def layer_init_ortho(layer, std=np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, std)\n",
    "  if layer.bias != None:\n",
    "    nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def layer_init_xavier(layer, gain):\n",
    "  nn.init.xavier_normal_(layer.weight, gain)\n",
    "  if layer.bias != None:\n",
    "    nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "  std = torch.exp(0.5 * logvar)\n",
    "  eps = torch.randn_like(std)\n",
    "  return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Load the dataset and create a dataloader for it\n",
    "with open(\"../\" + RECIPE_DATASET_FILENAME, 'rb') as f:\n",
    "  dataset = pickle.load(f)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14091"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAIN_TYPE_EMBED_SIZE         = 48\n",
    "ADJUNCT_TYPE_EMBED_SIZE       = 64\n",
    "HOP_TYPE_EMBED_SIZE           = 256\n",
    "MISC_TYPE_EMBED_SIZE          = 128\n",
    "MICROORGANISM_TYPE_EMBED_SIZE = 256\n",
    "\n",
    "# Embedding labels\n",
    "db_filepath = build_db_str(find_file_cwd_and_parent_dirs(BREWBRAIN_DB_FILENAME, os.getcwd()))\n",
    "engine = create_engine(db_filepath, echo=False, future=True)\n",
    "Base.metadata.create_all(engine)\n",
    "grain_type_embedding_labels = core_grain_labels(engine, dataset)\n",
    "adjunct_type_embedding_labels = core_adjunct_labels(engine, dataset)\n",
    "hop_type_embedding_labels = hop_labels(engine, dataset)\n",
    "misc_type_embedding_labels = misc_labels(engine, dataset)\n",
    "microorganism_type_embedding_labels = microorganism_labels(engine, dataset)\n",
    "\n",
    "class RecipeNetArgs:\n",
    "  def __init__(self, dataset: RecipeDataset) -> None:\n",
    "    # Recipe-specific constraints ***\n",
    "    self.num_mash_steps          = NUM_MASH_STEPS\n",
    "    self.num_grain_slots         = NUM_GRAIN_SLOTS\n",
    "    self.num_adjunct_slots       = NUM_ADJUNCT_SLOTS\n",
    "    self.num_hop_slots           = NUM_HOP_SLOTS\n",
    "    self.num_misc_slots          = NUM_MISC_SLOTS\n",
    "    self.num_microorganism_slots = NUM_MICROORGANISM_SLOTS\n",
    "    self.num_ferment_stage_slots = NUM_FERMENT_STAGE_SLOTS\n",
    "    \n",
    "    # NOTE: All types include a \"None\" (i.e., empty) category\n",
    "    self.num_grain_types         = len(dataset.core_grains_idx_to_dbid) # Number of (core) grain types (rows in the DB)\n",
    "    self.num_adjunct_types       = len(dataset.core_adjs_idx_to_dbid)   # Number of (core) adjunct types (rows in the DB)\n",
    "    self.num_hop_types           = len(dataset.hops_idx_to_dbid)        # Number of hop types (rows in the DB)\n",
    "    self.num_misc_types          = len(dataset.miscs_idx_to_dbid)       # Number of misc. types (rows in the DB)\n",
    "    self.num_microorganism_types = len(dataset.mos_idx_to_dbid)         # Number of microrganism types (rows in the DB)\n",
    "    \n",
    "    self.num_mash_step_types  = len(dataset.mash_step_idx_to_name)  # Number of mash step types (e.g., Infusion, Decoction, Temperature)\n",
    "    self.num_hop_stage_types  = len(dataset.hop_stage_idx_to_name)  # Number of hop stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_misc_stage_types = len(dataset.misc_stage_idx_to_name) # Number of misc stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_mo_stage_types   = len(dataset.mo_stage_idx_to_name)   # Number of microorganism stage types (e.g., Primary, Secondary)\n",
    "    \n",
    "    # Embedding sizes ***\n",
    "    self.grain_type_embed_size         = GRAIN_TYPE_EMBED_SIZE\n",
    "    self.adjunct_type_embed_size       = ADJUNCT_TYPE_EMBED_SIZE\n",
    "    self.hop_type_embed_size           = HOP_TYPE_EMBED_SIZE\n",
    "    self.misc_type_embed_size          = MISC_TYPE_EMBED_SIZE\n",
    "    self.microorganism_type_embed_size = MICROORGANISM_TYPE_EMBED_SIZE\n",
    "    \n",
    "    # Network-specific hyperparameters/constraints ***\n",
    "    self.hidden_layers = [2048]\n",
    "    self.z_size = 128 # Latent-bottleneck dimension\n",
    "    self.activation_fn = nn.ELU\n",
    "    self.gain = nn.init.calculate_gain('linear', None) # Make sure this corresponds to the activation function!\n",
    "\n",
    "    # VAE-specific hyperparameters ***\n",
    "    self.beta_vae_gamma = 1000\n",
    "    self.max_beta_vae_capacity = 25\n",
    "    self.beta_vae_C_stop_iter = 1e5\n",
    "  \n",
    "  @property\n",
    "  def num_toplvl_inputs(self):\n",
    "    # (boil_time + mash_ph + sparge_temp)\n",
    "    return 3 \n",
    "  @property\n",
    "  def num_mash_step_inputs(self):\n",
    "     # Mash steps (step_type_index_size + step_time + step_temp) * (number of slots) - ordering assumed [0: step 1, 1: step 2, etc.]\n",
    "    return self.num_mash_steps*(self.num_mash_step_types + 2)\n",
    "  @property\n",
    "  def num_ferment_stage_inputs(self):\n",
    "    # Fermentation stages (step_time + step_temp) * (number of stages) - ordering assumed [0: primary, 1: secondary]\n",
    "    return self.num_ferment_stage_slots*(2)\n",
    "  @property\n",
    "  def num_grain_slot_inputs(self):\n",
    "    # Grain/Malt bill slots (grain_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_grain_slots*(self.grain_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_adjunct_slot_inputs(self):\n",
    "    # Adjunct slots (adjunct_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_adjunct_slots*(self.adjunct_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_hop_slot_inputs(self):\n",
    "    # Hop slots (hop_type_embed_size + stage_type_index_size + time + concentration) * (number of slots) - no ordering\n",
    "    return self.num_hop_slots*(self.hop_type_embed_size + self.num_hop_stage_types + 2)\n",
    "  @property\n",
    "  def num_misc_slot_inputs(self):\n",
    "    # Misc. slots (misc_type_embed_size + stage_type_index_size + time + amounts) * (number of slots) - no ordering\n",
    "    return self.num_misc_slots*(self.misc_type_embed_size + self.num_misc_stage_types + 2)\n",
    "  @property\n",
    "  def num_microorganism_slot_inputs(self):\n",
    "    # Microorganism slots (mo_type_embed_size + stage_type_index_size) * (number of slots) - no ordering\n",
    "    return self.num_microorganism_slots*(self.microorganism_type_embed_size + self.num_mo_stage_types)\n",
    "  \n",
    "  @property\n",
    "  def num_inputs(self):\n",
    "    \"\"\"Determine the number of inputs to the network.\n",
    "    Returns:\n",
    "        int: The total number of network inputs.\n",
    "    \"\"\"\n",
    "    return self.num_toplvl_inputs + self.num_mash_step_inputs + self.num_ferment_stage_inputs + \\\n",
    "           self.num_grain_slot_inputs + self.num_adjunct_slot_inputs + self.num_hop_slot_inputs + \\\n",
    "           self.num_misc_slot_inputs + self.num_microorganism_slot_inputs   \n",
    "\n",
    "args = RecipeNetArgs(dataset)\n",
    "args.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeNetData(object):\n",
    "  def __init__(self) -> None:\n",
    "    pass\n",
    "  \n",
    "class RecipeNetHeadEncoder(nn.Module):\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    # Embeddings (NOTE: Any categoricals that don't have embeddings will be one-hot encoded)\n",
    "    self.grain_type_embedding         = nn.Embedding(args.num_grain_types, args.grain_type_embed_size)\n",
    "    self.adjunct_type_embedding       = nn.Embedding(args.num_adjunct_types, args.adjunct_type_embed_size) \n",
    "    self.hop_type_embedding           = nn.Embedding(args.num_hop_types, args.hop_type_embed_size)\n",
    "    self.misc_type_embedding          = nn.Embedding(args.num_misc_types, args.misc_type_embed_size)\n",
    "    self.microorganism_type_embedding = nn.Embedding(args.num_microorganism_types, args.microorganism_type_embed_size)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x):\n",
    "    heads = RecipeNetData()\n",
    "    # Simple top-level heads (high-level recipe parameters)\n",
    "    heads.x_toplvl = torch.cat((x['boil_time'].unsqueeze(1), x['mash_ph'].unsqueeze(1), x['sparge_temp'].unsqueeze(1)), dim=1) # (B, 3)\n",
    "    \n",
    "    # Mash step heads\n",
    "    # NOTE: Data shape is (B, S=number_of_mash_steps) for the\n",
    "    # following recipe tensors: {'mash_step_type_inds', 'mash_step_times', 'mash_step_avg_temps'}\n",
    "    num_mash_step_types = self.args.num_mash_step_types\n",
    "    heads.enc_mash_step_type_onehot = F.one_hot(x['mash_step_type_inds'].long(), num_mash_step_types).float().flatten(1) # (B, S, num_mash_step_types) -> (B, S*num_mash_step_types) = [B, 24]\n",
    "    heads.x_mash_steps = torch.cat((heads.enc_mash_step_type_onehot, x['mash_step_times'], x['mash_step_avg_temps']), dim=1) # (B, num_mash_step_types*S+S+S) = [B, 36=(24+6+6)]\n",
    "    \n",
    "    # Ferment stage heads\n",
    "    # NOTE: Data shape is (B, S=2) for the following recipe tensors: {'ferment_stage_times', 'ferment_stage_temps'}\n",
    "    heads.x_ferment_stages = torch.cat((x['ferment_stage_times'], x['ferment_stage_temps']), dim=1) # (B, S+S)\n",
    "\n",
    "    # Grain (malt bill) heads\n",
    "    # NOTE: Data shape is (B, S=num_grain_slots) for the following recipe tensors: {'grain_core_type_inds', 'grain_amts'}\n",
    "    num_grain_types = self.args.num_grain_types\n",
    "    heads.enc_grain_type_embed = self.grain_type_embedding(x['grain_core_type_inds']).flatten(1) # (B, S, grain_type_embed_size) -> (B, S*grain_type_embed_size)\n",
    "    heads.enc_grain_type_onehot = F.one_hot(x['grain_core_type_inds'].long(), num_grain_types).float() # (B, num_grain_slots, num_grain_types)\n",
    "    heads.x_grains = torch.cat((heads.enc_grain_type_embed, x['grain_amts']), dim=1) # (B, S*grain_type_embed_size+S)\n",
    "    \n",
    "    # Adjunct heads\n",
    "    # NOTE: Data shape is (B, S=num_adjunct_slots) for the following recipe tensors: {'adjunct_core_type_inds', 'adjunct_amts'}\n",
    "    num_adjunct_types = self.args.num_adjunct_types\n",
    "    heads.enc_adjunct_type_embed = self.adjunct_type_embedding(x['adjunct_core_type_inds']).flatten(1) # (B, S, adjunct_type_embed_size) -> (B, S*adjunct_type_embed_size)\n",
    "    heads.enc_adjunct_type_onehot = F.one_hot(x['adjunct_core_type_inds'].long(), num_adjunct_types).float() # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    heads.x_adjuncts = torch.cat((heads.enc_adjunct_type_embed, x['adjunct_amts']), dim=1) # (B, S*adjunct_type_embed_size+S)\n",
    "    \n",
    "    # Hop heads\n",
    "    # NOTE: Data shape is (B, S=num_hop_slots) for the following recipe tensors: \n",
    "    # {'hop_type_inds', 'hop_stage_type_inds', 'hop_times', 'hop_concentrations'}\n",
    "    num_hop_types = self.args.num_hop_types\n",
    "    num_hop_stage_types = self.args.num_hop_stage_types\n",
    "    heads.enc_hop_type_embed = self.hop_type_embedding(x['hop_type_inds']).flatten(1) # (B, S, hop_type_embed_size)\n",
    "    heads.enc_hop_type_onehot = F.one_hot(x['hop_type_inds'].long(), num_hop_types).float() # (B, num_hop_slots, num_hop_types)\n",
    "    heads.enc_hop_stage_type_onehot = F.one_hot(x['hop_stage_type_inds'].long(), num_hop_stage_types).float().flatten(1) # (B, S, num_hop_stage_types)\n",
    "    heads.x_hops = torch.cat((heads.enc_hop_type_embed, heads.enc_hop_stage_type_onehot, x['hop_times'], x['hop_concentrations']), dim=1) # (B, S*hop_type_embed_size + S*num_hop_stage_types + S + S)\n",
    "    \n",
    "    # Misc. heads\n",
    "    # NOTE: Data shape is (B, S=num_misc_slots) for the following recipe tensors:\n",
    "    # {'misc_type_inds', 'misc_stage_inds', 'misc_times', 'misc_amts'}\n",
    "    num_misc_types = self.args.num_misc_types\n",
    "    num_misc_stage_types = self.args.num_misc_stage_types\n",
    "    heads.enc_misc_type_embed = self.misc_type_embedding(x['misc_type_inds']).flatten(1) # (B, S, misc_type_embed_size)\n",
    "    heads.enc_misc_type_onehot = F.one_hot(x['misc_type_inds'].long(), num_misc_types).float() # (B, num_misc_slots, num_misc_types)\n",
    "    heads.enc_misc_stage_type_onehot = F.one_hot(x['misc_stage_inds'].long(), num_misc_stage_types).float().flatten(1) # (B, S, num_misc_stage_types)\n",
    "    heads.x_miscs = torch.cat((heads.enc_misc_type_embed, heads.enc_misc_stage_type_onehot, x['misc_times'], x['misc_amts']), dim=1) # (B, S*misc_type_embed_size + S*num_misc_stage_types + S + S)\n",
    "    \n",
    "    # Microorganism heads\n",
    "    # NOTE: Data shape is (B, S=num_microorganism_slots) for the following recipe tensors:\n",
    "    # {'mo_type_inds', 'mo_stage_inds'}\n",
    "    num_mo_types = self.args.num_microorganism_types\n",
    "    num_mo_stage_types = self.args.num_mo_stage_types\n",
    "    heads.enc_mo_type_embed = self.microorganism_type_embedding(x['mo_type_inds']).flatten(1) # (B, S, microorganism_type_embed_size)\n",
    "    heads.enc_mo_type_onehot = F.one_hot(x['mo_type_inds'].long(), num_mo_types).float() # (B, num_mo_slots, num_mo_types)\n",
    "    heads.enc_mo_stage_type_onehot = F.one_hot(x['mo_stage_inds'].long(), num_mo_stage_types).float().flatten(1) # (B, S, num_mo_stage_types)\n",
    "    heads.x_mos = torch.cat((heads.enc_mo_type_embed, heads.enc_mo_stage_type_onehot), dim=1) # (B, S*microorganism_type_embed_size + S*num_mo_stage_types)\n",
    "    \n",
    "    # Put all the recipe data together into a flattened tensor\n",
    "    x = torch.cat((heads.x_toplvl, heads.x_mash_steps, heads.x_ferment_stages, heads.x_grains, heads.x_adjuncts, heads.x_hops, heads.x_miscs, heads.x_mos), dim=1) # (B, num_inputs)\n",
    "    return x, heads\n",
    "\n",
    "class RecipeNetFootDecoder(nn.Module):\n",
    "  def __init__(self, args: RecipeNetArgs) -> None:\n",
    "    super().__init__()\n",
    "    gain = args.gain\n",
    "    self.grain_type_decoder         = layer_init_xavier(nn.Linear(args.grain_type_embed_size, args.num_grain_types, bias=False), gain)\n",
    "    self.adjunct_type_decoder       = layer_init_xavier(nn.Linear(args.adjunct_type_embed_size, args.num_adjunct_types, bias=False), gain)\n",
    "    self.hop_type_decoder           = layer_init_xavier(nn.Linear(args.hop_type_embed_size, args.num_hop_types, bias=False), gain)\n",
    "    self.misc_type_decoder          = layer_init_xavier(nn.Linear(args.misc_type_embed_size, args.num_misc_types, bias=False), gain)\n",
    "    self.microorganism_type_decoder = layer_init_xavier(nn.Linear(args.microorganism_type_embed_size, args.num_microorganism_types, bias=False), gain)\n",
    "    \n",
    "    # [Top-level recipe attributes, Mash steps, Fermentation stages, Grains, Adjuncts, Hops, Misc, Microorganisms]\n",
    "    self.split_sizes = [\n",
    "      args.num_toplvl_inputs, args.num_mash_step_inputs, args.num_ferment_stage_inputs, \n",
    "      args.num_grain_slot_inputs, args.num_adjunct_slot_inputs, args.num_hop_slot_inputs,\n",
    "      args.num_misc_slot_inputs, args.num_microorganism_slot_inputs\n",
    "    ]\n",
    "    #assert np.sum(se)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x_hat):\n",
    "    foots = RecipeNetData()\n",
    "    \n",
    "    # The decoded tensor is flat with a shape of (B, num_inputs), we'll need to break it apart\n",
    "    # so that we can eventually calculate losses appropriately for each head of original data fed to the encoder\n",
    "    foots.x_hat_toplvl, foots.x_hat_mash_steps, foots.x_hat_ferment_stages, foots.x_hat_grains, foots.x_hat_adjuncts, foots.x_hat_hops, foots.x_hat_miscs, foots.x_hat_mos = torch.split(x_hat, self.split_sizes, dim=1)\n",
    "\n",
    "    # Mash steps\n",
    "    num_mash_steps = self.args.num_mash_steps\n",
    "    enc_mash_step_type_onehot_size = num_mash_steps * self.args.num_mash_step_types\n",
    "    foots.dec_mash_step_type_onehot, foots.dec_mash_step_times, foots.dec_mash_step_avg_temps = torch.split(\n",
    "      foots.x_hat_mash_steps, [enc_mash_step_type_onehot_size, num_mash_steps, num_mash_steps], dim=1\n",
    "    )\n",
    "\n",
    "    # Grain slots\n",
    "    num_grain_slots = self.args.num_grain_slots\n",
    "    grain_type_embed_size = self.args.grain_type_embed_size\n",
    "    enc_grain_type_embed_size = num_grain_slots * grain_type_embed_size\n",
    "    foots.dec_grain_type_embed, foots.dec_grain_amts = torch.split(foots.x_hat_grains, [enc_grain_type_embed_size, num_grain_slots], dim=1)\n",
    "    foots.dec_grain_type_logits = self.grain_type_decoder(foots.dec_grain_type_embed.view(-1, num_grain_slots, grain_type_embed_size)) # (B, num_grain_slots, num_grain_types)\n",
    "\n",
    "    # Adjunct slots\n",
    "    num_adjunct_slots = self.args.num_adjunct_slots\n",
    "    adjunct_type_embed_size = self.args.adjunct_type_embed_size\n",
    "    enc_adjunct_type_embed_size = num_adjunct_slots * adjunct_type_embed_size\n",
    "    dec_adjunct_type_embed, foots.dec_adjunct_amts = torch.split(foots.x_hat_adjuncts, [enc_adjunct_type_embed_size, num_adjunct_slots], dim=1)\n",
    "    foots.dec_adjunct_type_logits = self.adjunct_type_decoder(dec_adjunct_type_embed.view(-1, num_adjunct_slots, adjunct_type_embed_size)) # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    \n",
    "    # Hop slots\n",
    "    num_hop_slots = self.args.num_hop_slots\n",
    "    hop_type_embed_size = self.args.hop_type_embed_size\n",
    "    enc_hop_type_embed_size = num_hop_slots * hop_type_embed_size\n",
    "    enc_hop_stage_type_onehot_size = num_hop_slots * self.args.num_hop_stage_types\n",
    "    dec_hop_type_embed, foots.dec_hop_stage_type_onehot, foots.dec_hop_times, foots.dec_hop_concentrations = torch.split(\n",
    "      foots.x_hat_hops, [enc_hop_type_embed_size, enc_hop_stage_type_onehot_size, num_hop_slots, num_hop_slots], dim=1\n",
    "    )\n",
    "    foots.dec_hop_type_logits = self.hop_type_decoder(dec_hop_type_embed.view(-1, num_hop_slots, hop_type_embed_size)) # (B, num_hop_slots, num_hop_types)\n",
    "    \n",
    "    # Miscellaneous slots\n",
    "    num_misc_slots = self.args.num_misc_slots\n",
    "    misc_type_embed_size = self.args.misc_type_embed_size\n",
    "    enc_misc_type_embed_size = num_misc_slots * misc_type_embed_size\n",
    "    enc_misc_stage_type_onehot_size = num_misc_slots * self.args.num_misc_stage_types\n",
    "    dec_misc_type_embed, foots.dec_misc_stage_type_onehot, foots.dec_misc_times, foots.dec_misc_amts = torch.split(\n",
    "      foots.x_hat_miscs, [enc_misc_type_embed_size, enc_misc_stage_type_onehot_size, num_misc_slots, num_misc_slots], dim=1\n",
    "    )\n",
    "    foots.dec_misc_type_logits = self.misc_type_decoder(dec_misc_type_embed.view(-1, num_misc_slots, misc_type_embed_size)) # (B, num_misc_slots, num_misc_types)\n",
    "    \n",
    "    # Microorganism slots\n",
    "    num_mo_slots = self.args.num_microorganism_slots\n",
    "    mo_type_embed_size = self.args.microorganism_type_embed_size\n",
    "    enc_mo_type_embed_size = num_mo_slots * mo_type_embed_size\n",
    "    enc_mo_stage_type_onehot_size = num_mo_slots * self.args.num_mo_stage_types\n",
    "    dec_mo_type_embed, foots.dec_mo_stage_type_onehot = torch.split(\n",
    "      foots.x_hat_mos, [enc_mo_type_embed_size, enc_mo_stage_type_onehot_size], dim=1\n",
    "    )\n",
    "    foots.dec_mo_type_logits = self.microorganism_type_decoder(dec_mo_type_embed.view(-1, num_mo_slots, mo_type_embed_size)) # (B, num_mo_slots, num_mo_types)\n",
    "    \n",
    "    return foots\n",
    "    \n",
    "\n",
    "class RecipeNet(nn.Module):\n",
    "\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    \n",
    "    hidden_layers = args.hidden_layers\n",
    "    z_size = args.z_size\n",
    "    activation_fn = args.activation_fn\n",
    "    gain = args.gain\n",
    "    \n",
    "    assert all([num_hidden > 0 for num_hidden in hidden_layers])\n",
    "    assert args.num_inputs >= 1\n",
    "    assert len(hidden_layers) >= 1\n",
    "    assert z_size >= 1 and z_size < args.num_inputs\n",
    "\n",
    "    # Encoder and decoder networks\n",
    "    self.encoder = nn.Sequential()\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(args.num_inputs, hidden_layers[0]), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    prev_hidden_size = hidden_layers[0]\n",
    "    for hidden_size in hidden_layers[1:]:\n",
    "      self.encoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "      prev_hidden_size = hidden_size\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, z_size*2), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    self.encoder.append(nn.BatchNorm1d(z_size*2))\n",
    "\n",
    "    self.decoder = nn.Sequential()\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(z_size, hidden_layers[-1]), gain))\n",
    "    self.decoder.append(activation_fn())\n",
    "    prev_hidden_size = hidden_layers[-1]\n",
    "    for hidden_size in reversed(hidden_layers[:-1]):\n",
    "      self.decoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "      prev_hidden_size = hidden_size\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(hidden_layers[0], args.num_inputs), gain))\n",
    "    \n",
    "    # Pre-net Encoder (Network 'Heads')\n",
    "    self.head_encoder = RecipeNetHeadEncoder(args)\n",
    "    # Post-net Decoder (Network 'Foots')\n",
    "    self.foot_decoder = RecipeNetFootDecoder(args)\n",
    "\n",
    "    self.gamma = args.beta_vae_gamma\n",
    "    self.C_stop_iter = args.beta_vae_C_stop_iter\n",
    "    self.C_max = torch.Tensor([args.max_beta_vae_capacity])\n",
    "    \n",
    "    self.args = args\n",
    "  \n",
    "  def _apply(self, fn):\n",
    "    super()._apply(fn)\n",
    "    self.C_max = fn(self.C_max)\n",
    "    return self\n",
    "\n",
    "  def encode(self, input: torch.Tensor):\n",
    "    # Start by breaking the given x apart into all the various heads/embeddings \n",
    "    # and concatenate them into a value that can be fed to the encoder network\n",
    "    x, heads = self.head_encoder(input)\n",
    "    # Encode to the latent distribution mean and std dev.\n",
    "    mean, logvar = torch.chunk(self.encoder(x), 2, dim=-1) \n",
    "    return heads, mean, logvar\n",
    "  \n",
    "  def decode(self, z: torch.Tensor):\n",
    "    # Decode to the flattened output\n",
    "    x_hat = self.decoder(z)\n",
    "    # We need to perform the reverse process on the output from the decoder network:\n",
    "    # Break apart the output into matching segments similar to the heads (foots!) for use in later loss calculations\n",
    "    foots = self.foot_decoder(x_hat)\n",
    "    return foots\n",
    "    \n",
    "  def forward(self, input: torch.Tensor, use_mean=False):\n",
    "    heads, mean, logvar = self.encode(input)\n",
    "    # Sample (reparameterize trick) the final latent vector (z)\n",
    "    z = mean if use_mean else reparameterize(mean, logvar)\n",
    "    foots = self.decode(z)\n",
    "\n",
    "    return heads, foots, mean, logvar\n",
    "  \n",
    "  def loss_fn(self, input, heads, foots, mean, logvar, num_iter, kl_weight=1.0):\n",
    "    REDUCTION = 'sum'\n",
    "    # TODO: Simplify all this stuff into fewer losses: \n",
    "    # Group together all BCELogit and MSE losses into singluar tensors in both x and x_hat\n",
    "    loss_toplvl = F.mse_loss(foots.x_hat_toplvl, heads.x_toplvl, reduction=REDUCTION)\n",
    "    loss_mash_steps = F.binary_cross_entropy_with_logits(foots.dec_mash_step_type_onehot, heads.enc_mash_step_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_mash_step_times, input['mash_step_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_mash_step_avg_temps, input['mash_step_avg_temps'], reduction=REDUCTION)\n",
    "    loss_ferment_stages = F.mse_loss(foots.x_hat_ferment_stages, heads.x_ferment_stages, reduction=REDUCTION)\n",
    "    loss_grains = F.binary_cross_entropy_with_logits(foots.dec_grain_type_logits, heads.enc_grain_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_grain_amts, input['grain_amts'], reduction=REDUCTION)\n",
    "    loss_adjuncts = F.binary_cross_entropy_with_logits(foots.dec_adjunct_type_logits, heads.enc_adjunct_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_adjunct_amts, input['adjunct_amts'], reduction=REDUCTION)\n",
    "    loss_hops = F.binary_cross_entropy_with_logits(foots.dec_hop_type_logits, heads.enc_hop_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_hop_stage_type_onehot, heads.enc_hop_stage_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_hop_times, input['hop_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_hop_concentrations, input['hop_concentrations'], reduction=REDUCTION)\n",
    "    loss_miscs = F.binary_cross_entropy_with_logits(foots.dec_misc_type_logits, heads.enc_misc_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_misc_stage_type_onehot, heads.enc_misc_stage_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_misc_times, input['misc_times'], reduction=REDUCTION) + \\\n",
    "      F.mse_loss(foots.dec_misc_amts, input['misc_amts'], reduction=REDUCTION)\n",
    "    loss_mos = F.binary_cross_entropy_with_logits(foots.dec_mo_type_logits, heads.enc_mo_type_onehot, reduction=REDUCTION) + \\\n",
    "      F.binary_cross_entropy_with_logits(foots.dec_mo_stage_type_onehot, heads.enc_mo_stage_type_onehot, reduction=REDUCTION)\n",
    "\n",
    "    # Add up all our losses for reconstruction of the recipe\n",
    "    reconst_loss = loss_toplvl + loss_mash_steps + loss_ferment_stages + loss_grains + loss_adjuncts + loss_hops + loss_miscs + loss_mos\n",
    "\n",
    "    # Beta-VAE KL calculation is based on https://arxiv.org/pdf/1804.03599.pdf\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1), dim=0)\n",
    "    C = torch.clamp(self.C_max/self.C_stop_iter * num_iter, 0, self.C_max.data[0])\n",
    "    loss = reconst_loss + kl_weight * self.gamma * (kl_loss - C).abs()\n",
    "    return loss, C\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "recipe_net = RecipeNet(args).to(device)\n",
    "optimizer  = torch.optim.Adam(recipe_net.parameters(), lr=1e-3, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1000, eps=1e-5)\n",
    "optimizer.param_groups[0]['lr'] = 1e-3 # Learning Rate\n",
    "global_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Global Step: 1119 Loss: 12184.4082 lr: 0.001 C: 0.3 \t\t\t\n",
      " Avg Epoch #1 loss: 127276.79981\t\t\t\t\n",
      " Global Step: 2237 Loss: 11851.26465 lr: 0.0001 C: 0.6 \t\t\n",
      " Avg Epoch #2 loss: 25375.02697\t\t\t\t\n",
      " Global Step: 3355 Loss: 10220.78809 lr: 1e-05 C: 0.8 \t\t\t\n",
      " Avg Epoch #3 loss: 58908.51901\t\t\t\t\n",
      " Global Step: 4473 Loss: 9501.39355 lr: 1e-05 C: 1.1 \t\t\t\n",
      " Avg Epoch #4 loss: 23705.84248\t\t\t\t\n",
      " Global Step: 5550 Loss: 21249.34375 lr: 1e-05 C: 1.4 \t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdbid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m   batch[key] \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 67\u001b[0m heads, foots, mean, logvar \u001b[39m=\u001b[39m recipe_net(batch)\n\u001b[1;32m     68\u001b[0m loss, C \u001b[39m=\u001b[39m recipe_net\u001b[39m.\u001b[39mloss_fn(batch, heads, foots, mean, logvar, global_step, KL_WEIGHT)\n\u001b[1;32m     70\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 231\u001b[0m, in \u001b[0;36mRecipeNet.forward\u001b[0;34m(self, input, use_mean)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor, use_mean\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 231\u001b[0m   heads, mean, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    232\u001b[0m   \u001b[39m# Sample (reparameterize trick) the final latent vector (z)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m   z \u001b[39m=\u001b[39m mean \u001b[39mif\u001b[39;00m use_mean \u001b[39melse\u001b[39;00m reparameterize(mean, logvar)\n",
      "Cell \u001b[0;32mIn[5], line 219\u001b[0m, in \u001b[0;36mRecipeNet.encode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m x, heads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_encoder(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[39m# Encode to the latent distribution mean and std dev.\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m mean, logvar \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x), \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m heads, mean, logvar\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(layer, input, output)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m# Distributions of weights of the first layer\u001b[39;00m\n\u001b[1;32m     38\u001b[0m first_layer \u001b[39m=\u001b[39m encoder_children[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m     39\u001b[0m first_layer\u001b[39m.\u001b[39mregister_forward_hook(\n\u001b[1;32m     40\u001b[0m   \u001b[39mlambda\u001b[39;00m layer, \u001b[39minput\u001b[39m, output:\n\u001b[0;32m---> 41\u001b[0m     writer\u001b[39m.\u001b[39;49madd_histogram(\u001b[39m\"\u001b[39;49m\u001b[39mdists/weights/encoder_first_layer\u001b[39;49m\u001b[39m\"\u001b[39;49m, layer\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mflatten(), global_step, bins\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mif\u001b[39;00m global_step \u001b[39m%\u001b[39m MONITOR_UPDATE_STEPS \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m last_layer \u001b[39m=\u001b[39m encoder_children[num_hidden_layers\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m     44\u001b[0m last_layer\u001b[39m.\u001b[39mregister_forward_hook(\n\u001b[1;32m     45\u001b[0m   \u001b[39mlambda\u001b[39;00m layer, \u001b[39minput\u001b[39m, output:\n\u001b[1;32m     46\u001b[0m     writer\u001b[39m.\u001b[39madd_histogram(\u001b[39m\"\u001b[39m\u001b[39mdists/weights/encoder_last_layer\u001b[39m\u001b[39m\"\u001b[39m, layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mflatten(), global_step, bins\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m global_step \u001b[39m%\u001b[39m MONITOR_UPDATE_STEPS \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m )\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:485\u001b[0m, in \u001b[0;36mSummaryWriter.add_histogram\u001b[0;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(bins, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m bins \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    483\u001b[0m     bins \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_bins\n\u001b[1;32m    484\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_file_writer()\u001b[39m.\u001b[39madd_summary(\n\u001b[0;32m--> 485\u001b[0m     histogram(tag, values, bins, max_bins\u001b[39m=\u001b[39;49mmax_bins), global_step, walltime\n\u001b[1;32m    486\u001b[0m )\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:358\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(name, values, bins, max_bins)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m\"\"\"Outputs a `Summary` protocol buffer with a histogram.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39mThe generated\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39m  buffer.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m values \u001b[39m=\u001b[39m make_np(values)\n\u001b[0;32m--> 358\u001b[0m hist \u001b[39m=\u001b[39m make_histogram(values\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m), bins, max_bins)\n\u001b[1;32m    359\u001b[0m \u001b[39mreturn\u001b[39;00m Summary(value\u001b[39m=\u001b[39m[Summary\u001b[39m.\u001b[39mValue(tag\u001b[39m=\u001b[39mname, histo\u001b[39m=\u001b[39mhist)])\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:367\u001b[0m, in \u001b[0;36mmake_histogram\u001b[0;34m(values, bins, max_bins)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe input has no element.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 367\u001b[0m counts, limits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhistogram(values, bins\u001b[39m=\u001b[39;49mbins)\n\u001b[1;32m    368\u001b[0m num_bins \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(counts)\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m max_bins \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m num_bins \u001b[39m>\u001b[39m max_bins:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/histograms.py:793\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \n\u001b[1;32m    790\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m a, weights \u001b[39m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 793\u001b[0m bin_edges, uniform_bins \u001b[39m=\u001b[39m _get_bin_edges(a, bins, \u001b[39mrange\u001b[39;49m, weights)\n\u001b[1;32m    795\u001b[0m \u001b[39m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/histograms.py:409\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    406\u001b[0m     n_equal_bins \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    407\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Do not call selectors on empty arrays\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     width \u001b[39m=\u001b[39m _hist_bin_selectors[bin_name](a, (first_edge, last_edge))\n\u001b[1;32m    410\u001b[0m     \u001b[39mif\u001b[39;00m width:\n\u001b[1;32m    411\u001b[0m         n_equal_bins \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(_unsigned_subtract(last_edge, first_edge) \u001b[39m/\u001b[39m width))\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/histograms.py:263\u001b[0m, in \u001b[0;36m_hist_bin_auto\u001b[0;34m(x, range)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_hist_bin_auto\u001b[39m(x, \u001b[39mrange\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m    Histogram bin estimator that uses the minimum width of the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m    Freedman-Diaconis and Sturges estimators if the FD bin width is non-zero.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m    _hist_bin_fd, _hist_bin_sturges\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     fd_bw \u001b[39m=\u001b[39m _hist_bin_fd(x, \u001b[39mrange\u001b[39;49m)\n\u001b[1;32m    264\u001b[0m     sturges_bw \u001b[39m=\u001b[39m _hist_bin_sturges(x, \u001b[39mrange\u001b[39m)\n\u001b[1;32m    265\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mrange\u001b[39m  \u001b[39m# unused\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/histograms.py:225\u001b[0m, in \u001b[0;36m_hist_bin_fd\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mThe Freedman-Diaconis histogram bin estimator.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mh : An estimate of the optimal bin width for the given data.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mrange\u001b[39m  \u001b[39m# unused\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m iqr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msubtract(\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mpercentile(x, [\u001b[39m75\u001b[39;49m, \u001b[39m25\u001b[39;49m]))\n\u001b[1;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m iqr \u001b[39m*\u001b[39m x\u001b[39m.\u001b[39msize \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39m3.0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/function_base.py:4166\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[1;32m   4165\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPercentiles must be in the range [0, 100]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4166\u001b[0m \u001b[39mreturn\u001b[39;00m _quantile_unchecked(\n\u001b[1;32m   4167\u001b[0m     a, q, axis, out, overwrite_input, method, keepdims)\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/function_base.py:4424\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[1;32m   4416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_quantile_unchecked\u001b[39m(a,\n\u001b[1;32m   4417\u001b[0m                         q,\n\u001b[1;32m   4418\u001b[0m                         axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4421\u001b[0m                         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   4422\u001b[0m                         keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   4423\u001b[0m     \u001b[39m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4424\u001b[0m     r, k \u001b[39m=\u001b[39m _ureduce(a,\n\u001b[1;32m   4425\u001b[0m                     func\u001b[39m=\u001b[39;49m_quantile_ureduce_func,\n\u001b[1;32m   4426\u001b[0m                     q\u001b[39m=\u001b[39;49mq,\n\u001b[1;32m   4427\u001b[0m                     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   4428\u001b[0m                     out\u001b[39m=\u001b[39;49mout,\n\u001b[1;32m   4429\u001b[0m                     overwrite_input\u001b[39m=\u001b[39;49moverwrite_input,\n\u001b[1;32m   4430\u001b[0m                     method\u001b[39m=\u001b[39;49mmethod)\n\u001b[1;32m   4431\u001b[0m     \u001b[39mif\u001b[39;00m keepdims:\n\u001b[1;32m   4432\u001b[0m         \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mreshape(q\u001b[39m.\u001b[39mshape \u001b[39m+\u001b[39m k)\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/function_base.py:3725\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3723\u001b[0m     keepdim \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m a\u001b[39m.\u001b[39mndim\n\u001b[0;32m-> 3725\u001b[0m r \u001b[39m=\u001b[39m func(a, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3726\u001b[0m \u001b[39mreturn\u001b[39;00m r, keepdim\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/function_base.py:4593\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4591\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4592\u001b[0m         arr \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m-> 4593\u001b[0m result \u001b[39m=\u001b[39m _quantile(arr,\n\u001b[1;32m   4594\u001b[0m                    quantiles\u001b[39m=\u001b[39;49mq,\n\u001b[1;32m   4595\u001b[0m                    axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   4596\u001b[0m                    method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   4597\u001b[0m                    out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m   4598\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/mnt/dev/anaconda3/envs/brewbrain/lib/python3.10/site-packages/numpy/lib/function_base.py:4691\u001b[0m, in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[1;32m   4687\u001b[0m previous_indexes, next_indexes \u001b[39m=\u001b[39m _get_indexes(arr,\n\u001b[1;32m   4688\u001b[0m                                               virtual_indexes,\n\u001b[1;32m   4689\u001b[0m                                               values_count)\n\u001b[1;32m   4690\u001b[0m \u001b[39m# --- Sorting\u001b[39;00m\n\u001b[0;32m-> 4691\u001b[0m arr\u001b[39m.\u001b[39;49mpartition(\n\u001b[1;32m   4692\u001b[0m     np\u001b[39m.\u001b[39;49munique(np\u001b[39m.\u001b[39;49mconcatenate(([\u001b[39m0\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m   4693\u001b[0m                               previous_indexes\u001b[39m.\u001b[39;49mravel(),\n\u001b[1;32m   4694\u001b[0m                               next_indexes\u001b[39m.\u001b[39;49mravel(),\n\u001b[1;32m   4695\u001b[0m                               ))),\n\u001b[1;32m   4696\u001b[0m     axis\u001b[39m=\u001b[39;49mDATA_AXIS)\n\u001b[1;32m   4697\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(arr\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minexact):\n\u001b[1;32m   4698\u001b[0m     slices_having_nans \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39misnan(\n\u001b[1;32m   4699\u001b[0m         take(arr, indices\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39mDATA_AXIS)\n\u001b[1;32m   4700\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "run_dir = os.path.join(\"../runs\", f\"recipe_vae_{int(time.time())}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "writer = SummaryWriter(run_dir)\n",
    "writer.add_text(\n",
    "  \"hyperparameters\",\n",
    "  \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()]))\n",
    ")\n",
    "\n",
    "# Monitor the recipe network using hooks and tensorboard\n",
    "MONITOR_UPDATE_STEPS = 1000\n",
    "for name, layer in recipe_net.named_children():\n",
    "  if name == 'encoder':\n",
    "    encoder_children = list(layer.named_children())\n",
    "    # Distributions of outputs after the first layer+activation\n",
    "    first_actfn = encoder_children[1][1]\n",
    "    first_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_first_actfn\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    # Distribution of outputs after the last layer+activation (before batchnorm)\n",
    "    num_hidden_layers = len(args.hidden_layers)\n",
    "    last_actfn = encoder_children[1+num_hidden_layers*2][1]\n",
    "    last_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_last_actfn\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    # Distribution of outputs after the encoder (last layer is a batchnorm1D)\n",
    "    batchnorm = encoder_children[2+num_hidden_layers*2][1]\n",
    "    batchnorm.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_batchnorm\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    \n",
    "    # Distributions of weights of the first layer\n",
    "    first_layer = encoder_children[0][1]\n",
    "    first_layer.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/weights/encoder_first_layer\", layer.weight.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    last_layer = encoder_children[num_hidden_layers*2][1]\n",
    "    last_layer.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/weights/encoder_last_layer\", layer.weight.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    \n",
    "  elif name == 'decoder':\n",
    "    pass\n",
    "  elif name == 'head_encoder':\n",
    "    pass\n",
    "  else: # name == 'foot_decoder':\n",
    "    pass\n",
    "\n",
    "KL_WEIGHT  = 1.0\n",
    "NUM_EPOCHS = 10\n",
    "outlier_ids = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "  epoch_loss = 0.0\n",
    "  for batch_idx, recipe_batch in enumerate(dataloader):\n",
    "    batch = {}\n",
    "    for key, value in recipe_batch.items():\n",
    "      if key == 'dbid': continue\n",
    "      batch[key] = value.cuda()\n",
    "\n",
    "    heads, foots, mean, logvar = recipe_net(batch)\n",
    "    loss, C = recipe_net.loss_fn(batch, heads, foots, mean, logvar, global_step-1, KL_WEIGHT)\n",
    "    \n",
    "    epoch_loss += loss.item()\n",
    "    writer.add_scalar(\"charts/total_loss\", loss.item(), global_step)\n",
    "    if global_step % 100 == 0:\n",
    "\n",
    "      \n",
    "    #if loss.item() > 3.5e4:\n",
    "    #  print(batch['dbid'])\n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(recipe_net.parameters(), 100.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    global_step += 1\n",
    "    \n",
    "    print('\\r', \"Global Step:\", global_step, \"Loss:\", np.around(loss.item(), 5), \"lr:\", optimizer.param_groups[0]['lr'], \"C:\", np.around(C.item(), 1), \"\\t\\t\", end='')\n",
    "      \n",
    "  # Send the head encoder's embeddings to tensorboard\n",
    "  writer.add_embedding(recipe_net.head_encoder.grain_type_embedding.weight, grain_type_embedding_labels, global_step=global_step, tag=\"embedding/grain_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.adjunct_type_embedding.weight, adjunct_type_embedding_labels, global_step=global_step, tag=\"embeddings/adjunct_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.hop_type_embedding.weight, hop_type_embedding_labels, global_step=global_step, tag=\"embeddings/hop_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.misc_type_embedding.weight, misc_type_embedding_labels, global_step=global_step, tag=\"embeddings/misc_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.microorganism_type_embedding.weight, microorganism_type_embedding_labels, global_step=global_step, tag=\"embeddings/microorganism_type\")        \n",
    "  writer.flush()\n",
    "    \n",
    "  avg_epoch_loss = epoch_loss / (batch_idx+1)\n",
    "  print(\"\\r\\n\", f\"Avg Epoch #{i+1} loss: {np.around(avg_epoch_loss,5)}\\t\\t\\t\\t\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brewbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f2e67b2e1dd515279f438694bfff88fe430fda25357a6c446b9da09bb563da9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
