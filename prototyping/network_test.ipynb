{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from brewbrain_db import Base, BREWBRAIN_DB_FILENAME, build_db_str\n",
    "from file_utils import find_file_cwd_and_parent_dirs\n",
    "from recipe_dataset import core_grain_labels, core_adjunct_labels, hop_labels, misc_labels, microorganism_labels\n",
    "from recipe_dataset import RecipeDataset, RECIPE_DATASET_FILENAME, NUM_GRAIN_SLOTS, NUM_ADJUNCT_SLOTS, NUM_HOP_SLOTS, NUM_MISC_SLOTS, NUM_MICROORGANISM_SLOTS, NUM_FERMENT_STAGE_SLOTS, NUM_MASH_STEPS\n",
    "from running_stats import RunningStats\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def layer_init_ortho(layer, std=np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, std)\n",
    "  if layer.bias != None:\n",
    "    nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def layer_init_xavier(layer, gain):\n",
    "  nn.init.xavier_normal_(layer.weight, gain)\n",
    "  if layer.bias != None:\n",
    "    nn.init.constant_(layer.bias, 0.0)\n",
    "  return layer\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "  std = torch.exp(0.5 * logvar)\n",
    "  eps = torch.randn_like(std)\n",
    "  return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Load the dataset and create a dataloader for it\n",
    "with open(\"../\" + RECIPE_DATASET_FILENAME, 'rb') as f:\n",
    "  dataset = pickle.load(f)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14091"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding labels\n",
    "db_filepath = build_db_str(find_file_cwd_and_parent_dirs(BREWBRAIN_DB_FILENAME, os.getcwd()))\n",
    "engine = create_engine(db_filepath, echo=False, future=True)\n",
    "Base.metadata.create_all(engine)\n",
    "grain_type_embedding_labels = core_grain_labels(engine, dataset)\n",
    "adjunct_type_embedding_labels = core_adjunct_labels(engine, dataset)\n",
    "hop_type_embedding_labels = hop_labels(engine, dataset)\n",
    "misc_type_embedding_labels = misc_labels(engine, dataset)\n",
    "microorganism_type_embedding_labels = microorganism_labels(engine, dataset)\n",
    "\n",
    "class RecipeNetArgs:\n",
    "  def __init__(self, dataset: RecipeDataset) -> None:\n",
    "    # Recipe-specific constraints ***\n",
    "    self.num_mash_steps          = NUM_MASH_STEPS\n",
    "    self.num_grain_slots         = NUM_GRAIN_SLOTS\n",
    "    self.num_adjunct_slots       = NUM_ADJUNCT_SLOTS\n",
    "    self.num_hop_slots           = NUM_HOP_SLOTS\n",
    "    self.num_misc_slots          = NUM_MISC_SLOTS\n",
    "    self.num_microorganism_slots = NUM_MICROORGANISM_SLOTS\n",
    "    self.num_ferment_stage_slots = NUM_FERMENT_STAGE_SLOTS\n",
    "    \n",
    "    # NOTE: All types include a \"None\" (i.e., empty) category\n",
    "    self.num_grain_types         = len(dataset.core_grains_idx_to_dbid) # Number of (core) grain types (rows in the DB)\n",
    "    self.num_adjunct_types       = len(dataset.core_adjs_idx_to_dbid)   # Number of (core) adjunct types (rows in the DB)\n",
    "    self.num_hop_types           = len(dataset.hops_idx_to_dbid)        # Number of hop types (rows in the DB)\n",
    "    self.num_misc_types          = len(dataset.miscs_idx_to_dbid)       # Number of misc. types (rows in the DB)\n",
    "    self.num_microorganism_types = len(dataset.mos_idx_to_dbid)         # Number of microrganism types (rows in the DB)\n",
    "    \n",
    "    self.num_mash_step_types  = len(dataset.mash_step_idx_to_name)  # Number of mash step types (e.g., Infusion, Decoction, Temperature)\n",
    "    self.num_hop_stage_types  = len(dataset.hop_stage_idx_to_name)  # Number of hop stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_misc_stage_types = len(dataset.misc_stage_idx_to_name) # Number of misc stage types (e.g., Mash, Boil, Primary, ...)\n",
    "    self.num_mo_stage_types   = len(dataset.mo_stage_idx_to_name)   # Number of microorganism stage types (e.g., Primary, Secondary)\n",
    "    \n",
    "    # Embedding sizes ***\n",
    "    self.grain_type_embed_size         = 48\n",
    "    self.adjunct_type_embed_size       = 64\n",
    "    self.hop_type_embed_size           = 256\n",
    "    self.misc_type_embed_size          = 128\n",
    "    self.microorganism_type_embed_size = 256\n",
    "    \n",
    "    # Network-specific hyperparameters/constraints ***\n",
    "    self.hidden_layers = [2048, 2048]\n",
    "    self.z_size = 128 # Latent-bottleneck dimension\n",
    "    self.activation_fn = nn.ELU\n",
    "    self.gain = nn.init.calculate_gain('linear', None) # Make sure this corresponds to the activation function!\n",
    "\n",
    "    # VAE-specific hyperparameters ***\n",
    "    self.beta_vae_gamma = 1000\n",
    "    self.max_beta_vae_capacity = 25\n",
    "    self.beta_vae_C_stop_iter = 1e5\n",
    "  \n",
    "  @property\n",
    "  def num_toplvl_inputs(self):\n",
    "    # (boil_time + mash_ph + sparge_temp)\n",
    "    return 3 \n",
    "  @property\n",
    "  def num_mash_step_inputs(self):\n",
    "     # Mash steps (step_type_index_size + step_time + step_temp) * (number of slots) - ordering assumed [0: step 1, 1: step 2, etc.]\n",
    "    return self.num_mash_steps*(self.num_mash_step_types + 2)\n",
    "  @property\n",
    "  def num_ferment_stage_inputs(self):\n",
    "    # Fermentation stages (step_time + step_temp) * (number of stages) - ordering assumed [0: primary, 1: secondary]\n",
    "    return self.num_ferment_stage_slots*(2)\n",
    "  @property\n",
    "  def num_grain_slot_inputs(self):\n",
    "    # Grain/Malt bill slots (grain_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_grain_slots*(self.grain_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_adjunct_slot_inputs(self):\n",
    "    # Adjunct slots (adjunct_type_embed_size + amount) * (number of slots) - no ordering\n",
    "    return self.num_adjunct_slots*(self.adjunct_type_embed_size + 1)\n",
    "  @property\n",
    "  def num_hop_slot_inputs(self):\n",
    "    # Hop slots (hop_type_embed_size + stage_type_index_size + time + concentration) * (number of slots) - no ordering\n",
    "    return self.num_hop_slots*(self.hop_type_embed_size + self.num_hop_stage_types + 2)\n",
    "  @property\n",
    "  def num_misc_slot_inputs(self):\n",
    "    # Misc. slots (misc_type_embed_size + stage_type_index_size + time + amounts) * (number of slots) - no ordering\n",
    "    return self.num_misc_slots*(self.misc_type_embed_size + self.num_misc_stage_types + 2)\n",
    "  @property\n",
    "  def num_microorganism_slot_inputs(self):\n",
    "    # Microorganism slots (mo_type_embed_size + stage_type_index_size) * (number of slots) - no ordering\n",
    "    return self.num_microorganism_slots*(self.microorganism_type_embed_size + self.num_mo_stage_types)\n",
    "  \n",
    "  @property\n",
    "  def num_inputs(self):\n",
    "    \"\"\"Determine the number of inputs to the network.\n",
    "    Returns:\n",
    "        int: The total number of network inputs.\n",
    "    \"\"\"\n",
    "    return self.num_toplvl_inputs + self.num_mash_step_inputs + self.num_ferment_stage_inputs + \\\n",
    "           self.num_grain_slot_inputs + self.num_adjunct_slot_inputs + self.num_hop_slot_inputs + \\\n",
    "           self.num_misc_slot_inputs + self.num_microorganism_slot_inputs   \n",
    "\n",
    "args = RecipeNetArgs(dataset)\n",
    "args.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeNetData(object):\n",
    "  def __init__(self) -> None:\n",
    "    pass\n",
    "  \n",
    "class RecipeNetHeadEncoder(nn.Module):\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    # Embeddings (NOTE: Any categoricals that don't have embeddings will be one-hot encoded)\n",
    "    self.grain_type_embedding         = nn.Embedding(args.num_grain_types, args.grain_type_embed_size)\n",
    "    self.adjunct_type_embedding       = nn.Embedding(args.num_adjunct_types, args.adjunct_type_embed_size) \n",
    "    self.hop_type_embedding           = nn.Embedding(args.num_hop_types, args.hop_type_embed_size)\n",
    "    self.misc_type_embedding          = nn.Embedding(args.num_misc_types, args.misc_type_embed_size)\n",
    "    self.microorganism_type_embedding = nn.Embedding(args.num_microorganism_types, args.microorganism_type_embed_size)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x):\n",
    "    heads = RecipeNetData()\n",
    "    # Simple top-level heads (high-level recipe parameters)\n",
    "    heads.x_toplvl = torch.cat((x['boil_time'].unsqueeze(1), x['mash_ph'].unsqueeze(1), x['sparge_temp'].unsqueeze(1)), dim=1) # (B, 3)\n",
    "    \n",
    "    # Mash step heads\n",
    "    # NOTE: Data shape is (B, S=number_of_mash_steps) for the\n",
    "    # following recipe tensors: {'mash_step_type_inds', 'mash_step_times', 'mash_step_avg_temps'}\n",
    "    num_mash_step_types = self.args.num_mash_step_types\n",
    "    heads.enc_mash_step_type_onehot = F.one_hot(x['mash_step_type_inds'].long(), num_mash_step_types).float().flatten(1) # (B, S, num_mash_step_types) -> (B, S*num_mash_step_types) = [B, 24]\n",
    "    heads.x_mash_steps = torch.cat((heads.enc_mash_step_type_onehot, x['mash_step_times'], x['mash_step_avg_temps']), dim=1) # (B, num_mash_step_types*S+S+S) = [B, 36=(24+6+6)]\n",
    "    \n",
    "    # Ferment stage heads\n",
    "    # NOTE: Data shape is (B, S=2) for the following recipe tensors: {'ferment_stage_times', 'ferment_stage_temps'}\n",
    "    heads.x_ferment_stages = torch.cat((x['ferment_stage_times'], x['ferment_stage_temps']), dim=1) # (B, S+S)\n",
    "\n",
    "    # Grain (malt bill) heads\n",
    "    # NOTE: Data shape is (B, S=num_grain_slots) for the following recipe tensors: {'grain_core_type_inds', 'grain_amts'}\n",
    "    num_grain_types = self.args.num_grain_types\n",
    "    heads.enc_grain_type_embed = self.grain_type_embedding(x['grain_core_type_inds']).flatten(1) # (B, S, grain_type_embed_size) -> (B, S*grain_type_embed_size)\n",
    "    heads.enc_grain_type_onehot = F.one_hot(x['grain_core_type_inds'].long(), num_grain_types).float() # (B, num_grain_slots, num_grain_types)\n",
    "    heads.x_grains = torch.cat((heads.enc_grain_type_embed, x['grain_amts']), dim=1) # (B, S*grain_type_embed_size+S)\n",
    "    \n",
    "    # Adjunct heads\n",
    "    # NOTE: Data shape is (B, S=num_adjunct_slots) for the following recipe tensors: {'adjunct_core_type_inds', 'adjunct_amts'}\n",
    "    num_adjunct_types = self.args.num_adjunct_types\n",
    "    heads.enc_adjunct_type_embed = self.adjunct_type_embedding(x['adjunct_core_type_inds']).flatten(1) # (B, S, adjunct_type_embed_size) -> (B, S*adjunct_type_embed_size)\n",
    "    heads.enc_adjunct_type_onehot = F.one_hot(x['adjunct_core_type_inds'].long(), num_adjunct_types).float() # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    heads.x_adjuncts = torch.cat((heads.enc_adjunct_type_embed, x['adjunct_amts']), dim=1) # (B, S*adjunct_type_embed_size+S)\n",
    "    \n",
    "    # Hop heads\n",
    "    # NOTE: Data shape is (B, S=num_hop_slots) for the following recipe tensors: \n",
    "    # {'hop_type_inds', 'hop_stage_type_inds', 'hop_times', 'hop_concentrations'}\n",
    "    num_hop_types = self.args.num_hop_types\n",
    "    num_hop_stage_types = self.args.num_hop_stage_types\n",
    "    heads.enc_hop_type_embed = self.hop_type_embedding(x['hop_type_inds']).flatten(1) # (B, S, hop_type_embed_size)\n",
    "    heads.enc_hop_type_onehot = F.one_hot(x['hop_type_inds'].long(), num_hop_types).float() # (B, num_hop_slots, num_hop_types)\n",
    "    heads.enc_hop_stage_type_onehot = F.one_hot(x['hop_stage_type_inds'].long(), num_hop_stage_types).float().flatten(1) # (B, S, num_hop_stage_types)\n",
    "    heads.x_hops = torch.cat((heads.enc_hop_type_embed, heads.enc_hop_stage_type_onehot, x['hop_times'], x['hop_concentrations']), dim=1) # (B, S*hop_type_embed_size + S*num_hop_stage_types + S + S)\n",
    "    \n",
    "    # Misc. heads\n",
    "    # NOTE: Data shape is (B, S=num_misc_slots) for the following recipe tensors:\n",
    "    # {'misc_type_inds', 'misc_stage_inds', 'misc_times', 'misc_amts'}\n",
    "    num_misc_types = self.args.num_misc_types\n",
    "    num_misc_stage_types = self.args.num_misc_stage_types\n",
    "    heads.enc_misc_type_embed = self.misc_type_embedding(x['misc_type_inds']).flatten(1) # (B, S, misc_type_embed_size)\n",
    "    heads.enc_misc_type_onehot = F.one_hot(x['misc_type_inds'].long(), num_misc_types).float() # (B, num_misc_slots, num_misc_types)\n",
    "    heads.enc_misc_stage_type_onehot = F.one_hot(x['misc_stage_inds'].long(), num_misc_stage_types).float().flatten(1) # (B, S, num_misc_stage_types)\n",
    "    heads.x_miscs = torch.cat((heads.enc_misc_type_embed, heads.enc_misc_stage_type_onehot, x['misc_times'], x['misc_amts']), dim=1) # (B, S*misc_type_embed_size + S*num_misc_stage_types + S + S)\n",
    "    \n",
    "    # Microorganism heads\n",
    "    # NOTE: Data shape is (B, S=num_microorganism_slots) for the following recipe tensors:\n",
    "    # {'mo_type_inds', 'mo_stage_inds'}\n",
    "    num_mo_types = self.args.num_microorganism_types\n",
    "    num_mo_stage_types = self.args.num_mo_stage_types\n",
    "    heads.enc_mo_type_embed = self.microorganism_type_embedding(x['mo_type_inds']).flatten(1) # (B, S, microorganism_type_embed_size)\n",
    "    heads.enc_mo_type_onehot = F.one_hot(x['mo_type_inds'].long(), num_mo_types).float() # (B, num_mo_slots, num_mo_types)\n",
    "    heads.enc_mo_stage_type_onehot = F.one_hot(x['mo_stage_inds'].long(), num_mo_stage_types).float().flatten(1) # (B, S, num_mo_stage_types)\n",
    "    heads.x_mos = torch.cat((heads.enc_mo_type_embed, heads.enc_mo_stage_type_onehot), dim=1) # (B, S*microorganism_type_embed_size + S*num_mo_stage_types)\n",
    "    \n",
    "    # Put all the recipe data together into a flattened tensor\n",
    "    x = torch.cat((heads.x_toplvl, heads.x_mash_steps, heads.x_ferment_stages, heads.x_grains, heads.x_adjuncts, heads.x_hops, heads.x_miscs, heads.x_mos), dim=1) # (B, num_inputs)\n",
    "    return x, heads\n",
    "\n",
    "class RecipeNetFootDecoder(nn.Module):\n",
    "  def __init__(self, args: RecipeNetArgs) -> None:\n",
    "    super().__init__()\n",
    "    gain = args.gain\n",
    "    self.grain_type_decoder         = layer_init_xavier(nn.Linear(args.grain_type_embed_size, args.num_grain_types, bias=False), gain)\n",
    "    self.adjunct_type_decoder       = layer_init_xavier(nn.Linear(args.adjunct_type_embed_size, args.num_adjunct_types, bias=False), gain)\n",
    "    self.hop_type_decoder           = layer_init_xavier(nn.Linear(args.hop_type_embed_size, args.num_hop_types, bias=False), gain)\n",
    "    self.misc_type_decoder          = layer_init_xavier(nn.Linear(args.misc_type_embed_size, args.num_misc_types, bias=False), gain)\n",
    "    self.microorganism_type_decoder = layer_init_xavier(nn.Linear(args.microorganism_type_embed_size, args.num_microorganism_types, bias=False), gain)\n",
    "    \n",
    "    # [Top-level recipe attributes, Mash steps, Fermentation stages, Grains, Adjuncts, Hops, Misc, Microorganisms]\n",
    "    self.split_sizes = [\n",
    "      args.num_toplvl_inputs, args.num_mash_step_inputs, args.num_ferment_stage_inputs, \n",
    "      args.num_grain_slot_inputs, args.num_adjunct_slot_inputs, args.num_hop_slot_inputs,\n",
    "      args.num_misc_slot_inputs, args.num_microorganism_slot_inputs\n",
    "    ]\n",
    "    #assert np.sum(se)\n",
    "    self.args = args\n",
    "    \n",
    "  def forward(self, x_hat):\n",
    "    foots = RecipeNetData()\n",
    "    \n",
    "    # The decoded tensor is flat with a shape of (B, num_inputs), we'll need to break it apart\n",
    "    # so that we can eventually calculate losses appropriately for each head of original data fed to the encoder\n",
    "    foots.x_hat_toplvl, foots.x_hat_mash_steps, foots.x_hat_ferment_stages, foots.x_hat_grains, foots.x_hat_adjuncts, foots.x_hat_hops, foots.x_hat_miscs, foots.x_hat_mos = torch.split(x_hat, self.split_sizes, dim=1)\n",
    "\n",
    "    # Mash steps\n",
    "    num_mash_steps = self.args.num_mash_steps\n",
    "    enc_mash_step_type_onehot_size = num_mash_steps * self.args.num_mash_step_types\n",
    "    foots.dec_mash_step_type_onehot, foots.dec_mash_step_times, foots.dec_mash_step_avg_temps = torch.split(\n",
    "      foots.x_hat_mash_steps, [enc_mash_step_type_onehot_size, num_mash_steps, num_mash_steps], dim=1\n",
    "    )\n",
    "\n",
    "    # Grain slots\n",
    "    num_grain_slots = self.args.num_grain_slots\n",
    "    grain_type_embed_size = self.args.grain_type_embed_size\n",
    "    enc_grain_type_embed_size = num_grain_slots * grain_type_embed_size\n",
    "    foots.dec_grain_type_embed, foots.dec_grain_amts = torch.split(foots.x_hat_grains, [enc_grain_type_embed_size, num_grain_slots], dim=1)\n",
    "    foots.dec_grain_type_logits = self.grain_type_decoder(foots.dec_grain_type_embed.view(-1, num_grain_slots, grain_type_embed_size)) # (B, num_grain_slots, num_grain_types)\n",
    "\n",
    "    # Adjunct slots\n",
    "    num_adjunct_slots = self.args.num_adjunct_slots\n",
    "    adjunct_type_embed_size = self.args.adjunct_type_embed_size\n",
    "    enc_adjunct_type_embed_size = num_adjunct_slots * adjunct_type_embed_size\n",
    "    dec_adjunct_type_embed, foots.dec_adjunct_amts = torch.split(foots.x_hat_adjuncts, [enc_adjunct_type_embed_size, num_adjunct_slots], dim=1)\n",
    "    foots.dec_adjunct_type_logits = self.adjunct_type_decoder(dec_adjunct_type_embed.view(-1, num_adjunct_slots, adjunct_type_embed_size)) # (B, num_adjunct_slots, num_adjunct_types)\n",
    "    \n",
    "    # Hop slots\n",
    "    num_hop_slots = self.args.num_hop_slots\n",
    "    hop_type_embed_size = self.args.hop_type_embed_size\n",
    "    enc_hop_type_embed_size = num_hop_slots * hop_type_embed_size\n",
    "    enc_hop_stage_type_onehot_size = num_hop_slots * self.args.num_hop_stage_types\n",
    "    dec_hop_type_embed, foots.dec_hop_stage_type_onehot, foots.dec_hop_times, foots.dec_hop_concentrations = torch.split(\n",
    "      foots.x_hat_hops, [enc_hop_type_embed_size, enc_hop_stage_type_onehot_size, num_hop_slots, num_hop_slots], dim=1\n",
    "    )\n",
    "    foots.dec_hop_type_logits = self.hop_type_decoder(dec_hop_type_embed.view(-1, num_hop_slots, hop_type_embed_size)) # (B, num_hop_slots, num_hop_types)\n",
    "    \n",
    "    # Miscellaneous slots\n",
    "    num_misc_slots = self.args.num_misc_slots\n",
    "    misc_type_embed_size = self.args.misc_type_embed_size\n",
    "    enc_misc_type_embed_size = num_misc_slots * misc_type_embed_size\n",
    "    enc_misc_stage_type_onehot_size = num_misc_slots * self.args.num_misc_stage_types\n",
    "    dec_misc_type_embed, foots.dec_misc_stage_type_onehot, foots.dec_misc_times, foots.dec_misc_amts = torch.split(\n",
    "      foots.x_hat_miscs, [enc_misc_type_embed_size, enc_misc_stage_type_onehot_size, num_misc_slots, num_misc_slots], dim=1\n",
    "    )\n",
    "    foots.dec_misc_type_logits = self.misc_type_decoder(dec_misc_type_embed.view(-1, num_misc_slots, misc_type_embed_size)) # (B, num_misc_slots, num_misc_types)\n",
    "    \n",
    "    # Microorganism slots\n",
    "    num_mo_slots = self.args.num_microorganism_slots\n",
    "    mo_type_embed_size = self.args.microorganism_type_embed_size\n",
    "    enc_mo_type_embed_size = num_mo_slots * mo_type_embed_size\n",
    "    enc_mo_stage_type_onehot_size = num_mo_slots * self.args.num_mo_stage_types\n",
    "    dec_mo_type_embed, foots.dec_mo_stage_type_onehot = torch.split(\n",
    "      foots.x_hat_mos, [enc_mo_type_embed_size, enc_mo_stage_type_onehot_size], dim=1\n",
    "    )\n",
    "    foots.dec_mo_type_logits = self.microorganism_type_decoder(dec_mo_type_embed.view(-1, num_mo_slots, mo_type_embed_size)) # (B, num_mo_slots, num_mo_types)\n",
    "    \n",
    "    return foots\n",
    "    \n",
    "\n",
    "class RecipeNet(nn.Module):\n",
    "\n",
    "  def __init__(self, args) -> None:\n",
    "    super().__init__()\n",
    "    \n",
    "    hidden_layers = args.hidden_layers\n",
    "    z_size = args.z_size\n",
    "    activation_fn = args.activation_fn\n",
    "    gain = args.gain\n",
    "    \n",
    "    assert all([num_hidden > 0 for num_hidden in hidden_layers])\n",
    "    assert args.num_inputs >= 1\n",
    "    assert len(hidden_layers) >= 1\n",
    "    assert z_size >= 1 and z_size < args.num_inputs\n",
    "\n",
    "    # Encoder and decoder networks\n",
    "    self.encoder = nn.Sequential()\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(args.num_inputs, hidden_layers[0]), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    prev_hidden_size = hidden_layers[0]\n",
    "    for hidden_size in hidden_layers[1:]:\n",
    "      self.encoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, hidden_size), gain))\n",
    "      self.encoder.append(activation_fn())\n",
    "      prev_hidden_size = hidden_size\n",
    "    self.encoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, z_size*2), gain))\n",
    "    self.encoder.append(activation_fn())\n",
    "    self.encoder.append(nn.BatchNorm1d(z_size*2))\n",
    "\n",
    "    self.decoder = nn.Sequential()\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(z_size, hidden_layers[-1]), gain))\n",
    "    self.decoder.append(activation_fn())\n",
    "    prev_hidden_size = hidden_layers[-1]\n",
    "    for hidden_size in reversed(hidden_layers[:-1]):\n",
    "      self.decoder.append(layer_init_xavier(nn.Linear(prev_hidden_size, hidden_size), gain))\n",
    "      self.decoder.append(activation_fn())\n",
    "      prev_hidden_size = hidden_size\n",
    "    self.decoder.append(layer_init_xavier(nn.Linear(hidden_layers[0], args.num_inputs), gain))\n",
    "    \n",
    "    # Pre-net Encoder (Network 'Heads')\n",
    "    self.head_encoder = RecipeNetHeadEncoder(args)\n",
    "    # Post-net Decoder (Network 'Foots')\n",
    "    self.foot_decoder = RecipeNetFootDecoder(args)\n",
    "\n",
    "    self.gamma = args.beta_vae_gamma\n",
    "    self.C_stop_iter = args.beta_vae_C_stop_iter\n",
    "    self.C_max = torch.Tensor([args.max_beta_vae_capacity])\n",
    "    \n",
    "    self.args = args\n",
    "  \n",
    "  def _apply(self, fn):\n",
    "    super()._apply(fn)\n",
    "    self.C_max = fn(self.C_max)\n",
    "    return self\n",
    "\n",
    "  def encode(self, input):\n",
    "    # Start by breaking the given x apart into all the various heads/embeddings \n",
    "    # and concatenate them into a value that can be fed to the encoder network\n",
    "    x, heads = self.head_encoder(input)\n",
    "    # Encode to the latent distribution mean and std dev.\n",
    "    mean, logvar = torch.chunk(self.encoder(x), 2, dim=-1) \n",
    "    return heads, mean, logvar\n",
    "  \n",
    "  def decode(self, z: torch.Tensor):\n",
    "    # Decode to the flattened output\n",
    "    x_hat = self.decoder(z)\n",
    "    # We need to perform the reverse process on the output from the decoder network:\n",
    "    # Break apart the output into matching segments similar to the heads (foots!) for use in later loss calculations\n",
    "    foots = self.foot_decoder(x_hat)\n",
    "    return foots\n",
    "    \n",
    "  def forward(self, input, use_mean=False):\n",
    "    heads, mean, logvar = self.encode(input)\n",
    "    # Sample (reparameterize trick) the final latent vector (z)\n",
    "    z = mean if use_mean else reparameterize(mean, logvar)\n",
    "    foots = self.decode(z)\n",
    "\n",
    "    return heads, foots, mean, logvar\n",
    "  \n",
    "  def loss_fn(self, input, heads, foots, mean, logvar, num_iter, kl_weight=1.0, reduction='sum'):\n",
    "    if reduction == 'none':\n",
    "      loss_wrap = lambda x: x.sum(dim=1)\n",
    "    else:\n",
    "      loss_wrap = lambda x: x\n",
    "\n",
    "    # TODO: Simplify all this stuff into fewer losses: \n",
    "    # Group together all BCELogit and MSE losses into singluar tensors in both x and x_hat\n",
    "    loss_toplvl = loss_wrap(F.mse_loss(foots.x_hat_toplvl, heads.x_toplvl, reduction=reduction))\n",
    "    loss_mash_steps = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_mash_step_type_onehot, heads.enc_mash_step_type_onehot, reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_mash_step_times, input['mash_step_times'], reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_mash_step_avg_temps, input['mash_step_avg_temps'], reduction=reduction))\n",
    "    loss_ferment_stages = loss_wrap(F.mse_loss(foots.x_hat_ferment_stages, heads.x_ferment_stages, reduction=reduction))\n",
    "    loss_grains = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_grain_type_logits.flatten(1), heads.enc_grain_type_onehot.flatten(1), reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_grain_amts, input['grain_amts'], reduction=reduction))\n",
    "    loss_adjuncts = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_adjunct_type_logits.flatten(1), heads.enc_adjunct_type_onehot.flatten(1), reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_adjunct_amts, input['adjunct_amts'], reduction=reduction))\n",
    "    loss_hops = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_hop_type_logits.flatten(1), heads.enc_hop_type_onehot.flatten(1), reduction=reduction)) + \\\n",
    "      loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_hop_stage_type_onehot, heads.enc_hop_stage_type_onehot, reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_hop_times, input['hop_times'], reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_hop_concentrations, input['hop_concentrations'], reduction=reduction))\n",
    "    loss_miscs = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_misc_type_logits.flatten(1), heads.enc_misc_type_onehot.flatten(1), reduction=reduction)) + \\\n",
    "      loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_misc_stage_type_onehot, heads.enc_misc_stage_type_onehot, reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_misc_times, input['misc_times'], reduction=reduction)) + \\\n",
    "      loss_wrap(F.mse_loss(foots.dec_misc_amts, input['misc_amts'], reduction=reduction))\n",
    "    loss_mos = loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_mo_type_logits.flatten(1), heads.enc_mo_type_onehot.flatten(1), reduction=reduction)) + \\\n",
    "      loss_wrap(F.binary_cross_entropy_with_logits(foots.dec_mo_stage_type_onehot, heads.enc_mo_stage_type_onehot, reduction=reduction))\n",
    "\n",
    "    # Add up all our losses for reconstruction of the recipe\n",
    "    reconst_loss = loss_toplvl + loss_mash_steps + loss_ferment_stages + loss_grains + loss_adjuncts + loss_hops + loss_miscs + loss_mos\n",
    "\n",
    "    # Beta-VAE KL calculation is based on https://arxiv.org/pdf/1804.03599.pdf\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mean ** 2 - logvar.exp(), dim=1), dim=0)\n",
    "    C = torch.clamp(self.C_max/self.C_stop_iter * num_iter, 0, self.C_max.data[0])\n",
    "    loss = reconst_loss + kl_weight * self.gamma * (kl_loss - C).abs()\n",
    "    return loss, C\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "recipe_net = RecipeNet(args).to(device)\n",
    "optimizer  = torch.optim.Adam(recipe_net.parameters(), lr=1e-3, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=4000, eps=1e-5)\n",
    "optimizer.param_groups[0]['lr'] = 1e-3 # Learning Rate\n",
    "global_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "run_dir = os.path.join(\"../runs\", f\"recipe_vae_{int(time.time())}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "writer = SummaryWriter(run_dir)\n",
    "\n",
    "writer.add_text(\n",
    "  \"hyperparameters\",\n",
    "  \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()]))\n",
    ")\n",
    "writer.add_text(\"Model Summary\", str(recipe_net).replace(\"\\n\", \"  \\n\"))\n",
    "\n",
    "running_loss = RunningStats()\n",
    "\n",
    "# Monitor the recipe network using hooks and tensorboard\n",
    "MONITOR_UPDATE_STEPS = 1000\n",
    "for name, layer in recipe_net.named_children():\n",
    "  if name == 'encoder':\n",
    "    encoder_children = list(layer.named_children())\n",
    "    # Distributions of outputs after the first layer+activation\n",
    "    first_actfn = encoder_children[1][1]\n",
    "    first_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_first_actfn\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    # Distribution of outputs after the last layer+activation (before batchnorm)\n",
    "    num_hidden_layers = len(args.hidden_layers)\n",
    "    last_actfn = encoder_children[1+num_hidden_layers*2][1]\n",
    "    last_actfn.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_last_actfn\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    # Distribution of outputs after the encoder (last layer is a batchnorm1D)\n",
    "    batchnorm = encoder_children[2+num_hidden_layers*2][1]\n",
    "    batchnorm.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/outputs/encoder_batchnorm\", output.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    \n",
    "    # Distributions of weights of the first layer\n",
    "    first_layer = encoder_children[0][1]\n",
    "    first_layer.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/weights/encoder_first_layer\", layer.weight.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    last_layer = encoder_children[num_hidden_layers*2][1]\n",
    "    last_layer.register_forward_hook(\n",
    "      lambda layer, input, output:\n",
    "        writer.add_histogram(\"dists/weights/encoder_last_layer\", layer.weight.flatten(), global_step, bins='auto') if global_step % MONITOR_UPDATE_STEPS == 0 else None\n",
    "    )\n",
    "    \n",
    "  elif name == 'decoder':\n",
    "    pass\n",
    "  elif name == 'head_encoder':\n",
    "    pass\n",
    "  else: # name == 'foot_decoder':\n",
    "    pass\n",
    "\n",
    "KL_WEIGHT  = 1.0\n",
    "NUM_EPOCHS = 100\n",
    "OUTLIER_MIN_LOSS = 3.5e4\n",
    "OUTLIER_PER_RECIPE_MIN_LOSS = 1e4\n",
    "outliers = {}\n",
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "\n",
    "  for batch_idx, recipe_batch in enumerate(dataloader):\n",
    "    batch = {}\n",
    "    for key, value in recipe_batch.items():\n",
    "      if key == 'dbid': continue\n",
    "      batch[key] = value.cuda()\n",
    "\n",
    "    heads, foots, mean, logvar = recipe_net(batch)\n",
    "    loss, C = recipe_net.loss_fn(batch, heads, foots, mean, logvar, global_step-1, KL_WEIGHT)\n",
    "    \n",
    "    running_loss.add(loss.item())\n",
    "    writer.add_scalar(\"charts/total_loss\", loss.item(), global_step)\n",
    "      \n",
    "    if epoch_idx > 0 and loss.item() > OUTLIER_MIN_LOSS:\n",
    "      with torch.no_grad():\n",
    "        # Go through each item in the batch and see what its loss is\n",
    "        full_loss, _ = recipe_net.loss_fn(batch, heads, foots, mean, logvar, global_step-1, KL_WEIGHT, 'none')\n",
    "        # Find the Database IDs for the recipes with the worst losses:\n",
    "        # Sort the losses with their indices, highest to lowest\n",
    "        loss_dict = {idx: loss.item() for idx, loss in enumerate(full_loss)}\n",
    "        ordered_idx_loss_tuples = sorted(loss_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        dbids = recipe_batch['dbid']\n",
    "        for idx, loss_val in ordered_idx_loss_tuples:\n",
    "          if loss_val <= OUTLIER_PER_RECIPE_MIN_LOSS: break\n",
    "          dbid = dbids[idx].item()\n",
    "          if dbid not in outliers:\n",
    "            outliers[dbid] = {'loss': loss_val, 'count': 1}\n",
    "          else:\n",
    "            outliers[dbid]['count'] += 1\n",
    "        \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(recipe_net.parameters(), 100.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    global_step += 1\n",
    "    \n",
    "    print('\\r', \"Global Step:\", global_step, \"Loss:\", np.around(loss.item(), 5), \"lr:\", optimizer.param_groups[0]['lr'], \"C:\", np.around(C.item(), 1), \"\\t\\t\", end='')\n",
    "      \n",
    "  # Send the head encoder's embeddings to tensorboard\n",
    "  writer.add_embedding(recipe_net.head_encoder.grain_type_embedding.weight, grain_type_embedding_labels, global_step=global_step, tag=\"grain_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.adjunct_type_embedding.weight, adjunct_type_embedding_labels, global_step=global_step, tag=\"adjunct_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.hop_type_embedding.weight, hop_type_embedding_labels, global_step=global_step, tag=\"hop_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.misc_type_embedding.weight, misc_type_embedding_labels, global_step=global_step, tag=\"misc_type\")\n",
    "  writer.add_embedding(recipe_net.head_encoder.microorganism_type_embedding.weight, microorganism_type_embedding_labels, global_step=global_step, tag=\"microorganism_type\")        \n",
    "  writer.flush()\n",
    "    \n",
    "  print(\"\\r\\n\", f\"Epoch #{epoch_idx+1} Running Loss: [Mean: {np.around(running_loss.mean(), 3)}, StdDev: {np.around(running_loss.std(), 3)}]\\t\\t\\t\\t\")\n",
    "  if len(outliers) > 0:\n",
    "    print(\"\", f\"Current outliers: {sorted(sorted(outliers.items(), key=lambda x: x[1]['loss'], reverse=True), key=lambda x: x[1]['count'], reverse=True)[:min(len(outliers),25)]}\")\n",
    "  running_loss.clear()\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brewbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f2e67b2e1dd515279f438694bfff88fe430fda25357a6c446b9da09bb563da9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
